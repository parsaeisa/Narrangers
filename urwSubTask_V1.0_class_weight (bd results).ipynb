{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers import EvalPrediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_ID</th>\n",
       "      <th>High_Level_Narratives_List</th>\n",
       "      <th>Sub_Narratives_List</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EN_CC_100000.txt</td>\n",
       "      <td>['CC: Controversy about green technologies', '...</td>\n",
       "      <td>['CC: Criticism of institutions and authoritie...</td>\n",
       "      <td>Pentagon plans to serve LAB-GROWN MEAT to troo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EN_CC_100002.txt</td>\n",
       "      <td>['CC: Criticism of institutions and authoritie...</td>\n",
       "      <td>['CC: Criticism of institutions and authoritie...</td>\n",
       "      <td>Oxford Residents Mount Resistance Against the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EN_CC_100003.txt</td>\n",
       "      <td>['CC: Criticism of climate movement', 'CC: Cri...</td>\n",
       "      <td>['CC: Criticism of institutions and authoritie...</td>\n",
       "      <td>Fonda Heads To Canada For Oil Sands Protest, M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EN_CC_100004.txt</td>\n",
       "      <td>['CC: Criticism of institutions and authoritie...</td>\n",
       "      <td>['CC: Controversy about green technologies: Ot...</td>\n",
       "      <td>A Tesla Owner Just Exposed A Sick Secret About...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EN_CC_100005.txt</td>\n",
       "      <td>['CC: Criticism of climate movement']</td>\n",
       "      <td>['CC: Criticism of climate movement: Climate m...</td>\n",
       "      <td>Climate Crazies Fail in Attempt to Vandalize A...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Document_ID                         High_Level_Narratives_List  \\\n",
       "0  EN_CC_100000.txt  ['CC: Controversy about green technologies', '...   \n",
       "1  EN_CC_100002.txt  ['CC: Criticism of institutions and authoritie...   \n",
       "2  EN_CC_100003.txt  ['CC: Criticism of climate movement', 'CC: Cri...   \n",
       "3  EN_CC_100004.txt  ['CC: Criticism of institutions and authoritie...   \n",
       "4  EN_CC_100005.txt              ['CC: Criticism of climate movement']   \n",
       "\n",
       "                                 Sub_Narratives_List  \\\n",
       "0  ['CC: Criticism of institutions and authoritie...   \n",
       "1  ['CC: Criticism of institutions and authoritie...   \n",
       "2  ['CC: Criticism of institutions and authoritie...   \n",
       "3  ['CC: Controversy about green technologies: Ot...   \n",
       "4  ['CC: Criticism of climate movement: Climate m...   \n",
       "\n",
       "                                                Text  \n",
       "0  Pentagon plans to serve LAB-GROWN MEAT to troo...  \n",
       "1  Oxford Residents Mount Resistance Against the ...  \n",
       "2  Fonda Heads To Canada For Oil Sands Protest, M...  \n",
       "3  A Tesla Owner Just Exposed A Sick Secret About...  \n",
       "4  Climate Crazies Fail in Attempt to Vandalize A...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data_with_text.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_ID</th>\n",
       "      <th>High_Level_Narrative</th>\n",
       "      <th>Sub_Narratives_List</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EN_CC_100000.txt</td>\n",
       "      <td>CC: Controversy about green technologies</td>\n",
       "      <td>['CC: Criticism of institutions and authoritie...</td>\n",
       "      <td>Pentagon plans to serve LAB-GROWN MEAT to troo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EN_CC_100000.txt</td>\n",
       "      <td>CC: Criticism of institutions and authorities</td>\n",
       "      <td>['CC: Criticism of institutions and authoritie...</td>\n",
       "      <td>Pentagon plans to serve LAB-GROWN MEAT to troo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EN_CC_100000.txt</td>\n",
       "      <td>CC: Hidden plots by secret schemes of powerful...</td>\n",
       "      <td>['CC: Criticism of institutions and authoritie...</td>\n",
       "      <td>Pentagon plans to serve LAB-GROWN MEAT to troo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EN_CC_100002.txt</td>\n",
       "      <td>CC: Criticism of institutions and authorities</td>\n",
       "      <td>['CC: Criticism of institutions and authoritie...</td>\n",
       "      <td>Oxford Residents Mount Resistance Against the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EN_CC_100002.txt</td>\n",
       "      <td>CC: Hidden plots by secret schemes of powerful...</td>\n",
       "      <td>['CC: Criticism of institutions and authoritie...</td>\n",
       "      <td>Oxford Residents Mount Resistance Against the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Document_ID                               High_Level_Narrative  \\\n",
       "0  EN_CC_100000.txt           CC: Controversy about green technologies   \n",
       "0  EN_CC_100000.txt      CC: Criticism of institutions and authorities   \n",
       "0  EN_CC_100000.txt  CC: Hidden plots by secret schemes of powerful...   \n",
       "1  EN_CC_100002.txt      CC: Criticism of institutions and authorities   \n",
       "1  EN_CC_100002.txt  CC: Hidden plots by secret schemes of powerful...   \n",
       "\n",
       "                                 Sub_Narratives_List  \\\n",
       "0  ['CC: Criticism of institutions and authoritie...   \n",
       "0  ['CC: Criticism of institutions and authoritie...   \n",
       "0  ['CC: Criticism of institutions and authoritie...   \n",
       "1  ['CC: Criticism of institutions and authoritie...   \n",
       "1  ['CC: Criticism of institutions and authoritie...   \n",
       "\n",
       "                                                Text  \n",
       "0  Pentagon plans to serve LAB-GROWN MEAT to troo...  \n",
       "0  Pentagon plans to serve LAB-GROWN MEAT to troo...  \n",
       "0  Pentagon plans to serve LAB-GROWN MEAT to troo...  \n",
       "1  Oxford Residents Mount Resistance Against the ...  \n",
       "1  Oxford Residents Mount Resistance Against the ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the 'High_Level_Narratives_List' column from string to list\n",
    "df['High_Level_Narratives_List'] = df['High_Level_Narratives_List'].apply(lambda x: eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "# Explode the dataframe to have one row per High Level Narrative\n",
    "df = df.explode('High_Level_Narratives_List')\n",
    "\n",
    "# Rename the column for clarity\n",
    "df = df.rename(columns={\"High_Level_Narratives_List\": \"High_Level_Narrative\"})\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_ID</th>\n",
       "      <th>High_Level_Narrative</th>\n",
       "      <th>Sub_Narratives_List</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EN_UA_000923.txt</td>\n",
       "      <td>URW: Speculating war outcomes</td>\n",
       "      <td>['URW: Discrediting the West, Diplomacy: The E...</td>\n",
       "      <td>Boris Johnson demands Putin ‘steps back from t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EN_UA_000923.txt</td>\n",
       "      <td>URW: Discrediting the West, Diplomacy</td>\n",
       "      <td>['URW: Discrediting the West, Diplomacy: The E...</td>\n",
       "      <td>Boris Johnson demands Putin ‘steps back from t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EN_UA_001032.txt</td>\n",
       "      <td>URW: Discrediting the West, Diplomacy</td>\n",
       "      <td>['URW: Discrediting the West, Diplomacy: Diplo...</td>\n",
       "      <td>Russia-Ukraine war map: Where are Russian troo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EN_UA_001052.txt</td>\n",
       "      <td>URW: Blaming the war on others rather than the...</td>\n",
       "      <td>['URW: Blaming the war on others rather than t...</td>\n",
       "      <td>NATO ‘Cautiously Optimistic’ Amid Reports of R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EN_UA_002668.txt</td>\n",
       "      <td>URW: Speculating war outcomes</td>\n",
       "      <td>['URW: Speculating war outcomes: Other', 'URW:...</td>\n",
       "      <td>Putin may ABANDON siege of Kyiv and try to bli...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Document_ID                               High_Level_Narrative  \\\n",
       "0  EN_UA_000923.txt                      URW: Speculating war outcomes   \n",
       "1  EN_UA_000923.txt              URW: Discrediting the West, Diplomacy   \n",
       "2  EN_UA_001032.txt              URW: Discrediting the West, Diplomacy   \n",
       "3  EN_UA_001052.txt  URW: Blaming the war on others rather than the...   \n",
       "4  EN_UA_002668.txt                      URW: Speculating war outcomes   \n",
       "\n",
       "                                 Sub_Narratives_List  \\\n",
       "0  ['URW: Discrediting the West, Diplomacy: The E...   \n",
       "1  ['URW: Discrediting the West, Diplomacy: The E...   \n",
       "2  ['URW: Discrediting the West, Diplomacy: Diplo...   \n",
       "3  ['URW: Blaming the war on others rather than t...   \n",
       "4  ['URW: Speculating war outcomes: Other', 'URW:...   \n",
       "\n",
       "                                                Text  \n",
       "0  Boris Johnson demands Putin ‘steps back from t...  \n",
       "1  Boris Johnson demands Putin ‘steps back from t...  \n",
       "2  Russia-Ukraine war map: Where are Russian troo...  \n",
       "3  NATO ‘Cautiously Optimistic’ Amid Reports of R...  \n",
       "4  Putin may ABANDON siege of Kyiv and try to bli...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df[\"High_Level_Narrative\"].str.startswith(\"URW\")].reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 264 URW High Level Narratives after spliting multiple URW High Level Narratives in multiple rows\n"
     ]
    }
   ],
   "source": [
    "print(f'We have {len(df)} URW High Level Narratives after spliting multiple URW High Level Narratives in multiple rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_ID</th>\n",
       "      <th>High_Level_Narrative</th>\n",
       "      <th>Sub_Narratives_List</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EN_UA_000923.txt</td>\n",
       "      <td>URW: Speculating war outcomes</td>\n",
       "      <td>['URW: Discrediting the West, Diplomacy: The E...</td>\n",
       "      <td>URW: Speculating war outcomes | Boris Johnson ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EN_UA_000923.txt</td>\n",
       "      <td>URW: Discrediting the West, Diplomacy</td>\n",
       "      <td>['URW: Discrediting the West, Diplomacy: The E...</td>\n",
       "      <td>URW: Discrediting the West, Diplomacy | Boris ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EN_UA_001032.txt</td>\n",
       "      <td>URW: Discrediting the West, Diplomacy</td>\n",
       "      <td>['URW: Discrediting the West, Diplomacy: Diplo...</td>\n",
       "      <td>URW: Discrediting the West, Diplomacy | Russia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EN_UA_001052.txt</td>\n",
       "      <td>URW: Blaming the war on others rather than the...</td>\n",
       "      <td>['URW: Blaming the war on others rather than t...</td>\n",
       "      <td>URW: Blaming the war on others rather than the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EN_UA_002668.txt</td>\n",
       "      <td>URW: Speculating war outcomes</td>\n",
       "      <td>['URW: Speculating war outcomes: Other', 'URW:...</td>\n",
       "      <td>URW: Speculating war outcomes | Putin may ABAN...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Document_ID                               High_Level_Narrative  \\\n",
       "0  EN_UA_000923.txt                      URW: Speculating war outcomes   \n",
       "1  EN_UA_000923.txt              URW: Discrediting the West, Diplomacy   \n",
       "2  EN_UA_001032.txt              URW: Discrediting the West, Diplomacy   \n",
       "3  EN_UA_001052.txt  URW: Blaming the war on others rather than the...   \n",
       "4  EN_UA_002668.txt                      URW: Speculating war outcomes   \n",
       "\n",
       "                                 Sub_Narratives_List  \\\n",
       "0  ['URW: Discrediting the West, Diplomacy: The E...   \n",
       "1  ['URW: Discrediting the West, Diplomacy: The E...   \n",
       "2  ['URW: Discrediting the West, Diplomacy: Diplo...   \n",
       "3  ['URW: Blaming the war on others rather than t...   \n",
       "4  ['URW: Speculating war outcomes: Other', 'URW:...   \n",
       "\n",
       "                                                Text  \n",
       "0  URW: Speculating war outcomes | Boris Johnson ...  \n",
       "1  URW: Discrediting the West, Diplomacy | Boris ...  \n",
       "2  URW: Discrediting the West, Diplomacy | Russia...  \n",
       "3  URW: Blaming the war on others rather than the...  \n",
       "4  URW: Speculating war outcomes | Putin may ABAN...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#in here we will add high level narrative to text with a | seperating high level narrative and text\n",
    "df[\"Text\"] = df[\"High_Level_Narrative\"] + \" | \" + df[\"Text\"]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                 [URW: Speculating war outcomes: Other]\n",
       "1      [URW: Discrediting the West, Diplomacy: The EU...\n",
       "2      [URW: Discrediting the West, Diplomacy: Diplom...\n",
       "3      [URW: Blaming the war on others rather than th...\n",
       "4      [URW: Speculating war outcomes: Other, URW: Sp...\n",
       "                             ...                        \n",
       "259    [URW: Praise of Russia: Praise of Russian mili...\n",
       "260    [URW: Amplifying war-related fears: By continu...\n",
       "261    [URW: Negative Consequences for the West: Sanc...\n",
       "262    [URW: Discrediting the West, Diplomacy: Diplom...\n",
       "263                   [URW: Russia is the Victim: Other]\n",
       "Name: Filtered_Sub_Narratives, Length: 264, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to filter sub-narratives that match the high-level narrative\n",
    "def filter_sub_narratives(row):\n",
    "    high_level = row[\"High_Level_Narrative\"].strip()\n",
    "    sub_narratives = eval(row[\"Sub_Narratives_List\"])  # Convert string representation of list to actual list\n",
    "    \n",
    "    # Keep only sub-narratives that start with the high-level narrative\n",
    "    filtered = [sub for sub in sub_narratives if sub.startswith(high_level)]\n",
    "    return filtered\n",
    "\n",
    "# Apply filtering function\n",
    "df[\"Filtered_Sub_Narratives\"] = df.apply(filter_sub_narratives, axis=1)\n",
    "df['Filtered_Sub_Narratives']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all unique sub-narratives\n",
    "sub_narratives = list(set(label for sublist in df[\"Filtered_Sub_Narratives\"] for label in sublist))\n",
    "len(sub_narratives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_ID</th>\n",
       "      <th>High_Level_Narrative</th>\n",
       "      <th>Sub_Narratives_List</th>\n",
       "      <th>Text</th>\n",
       "      <th>Filtered_Sub_Narratives</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EN_UA_000923.txt</td>\n",
       "      <td>URW: Speculating war outcomes</td>\n",
       "      <td>['URW: Discrediting the West, Diplomacy: The E...</td>\n",
       "      <td>URW: Speculating war outcomes | Boris Johnson ...</td>\n",
       "      <td>[URW: Speculating war outcomes: Other]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EN_UA_000923.txt</td>\n",
       "      <td>URW: Discrediting the West, Diplomacy</td>\n",
       "      <td>['URW: Discrediting the West, Diplomacy: The E...</td>\n",
       "      <td>URW: Discrediting the West, Diplomacy | Boris ...</td>\n",
       "      <td>[URW: Discrediting the West, Diplomacy: The EU...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EN_UA_001032.txt</td>\n",
       "      <td>URW: Discrediting the West, Diplomacy</td>\n",
       "      <td>['URW: Discrediting the West, Diplomacy: Diplo...</td>\n",
       "      <td>URW: Discrediting the West, Diplomacy | Russia...</td>\n",
       "      <td>[URW: Discrediting the West, Diplomacy: Diplom...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EN_UA_001052.txt</td>\n",
       "      <td>URW: Blaming the war on others rather than the...</td>\n",
       "      <td>['URW: Blaming the war on others rather than t...</td>\n",
       "      <td>URW: Blaming the war on others rather than the...</td>\n",
       "      <td>[URW: Blaming the war on others rather than th...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EN_UA_002668.txt</td>\n",
       "      <td>URW: Speculating war outcomes</td>\n",
       "      <td>['URW: Speculating war outcomes: Other', 'URW:...</td>\n",
       "      <td>URW: Speculating war outcomes | Putin may ABAN...</td>\n",
       "      <td>[URW: Speculating war outcomes: Other, URW: Sp...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Document_ID                               High_Level_Narrative  \\\n",
       "0  EN_UA_000923.txt                      URW: Speculating war outcomes   \n",
       "1  EN_UA_000923.txt              URW: Discrediting the West, Diplomacy   \n",
       "2  EN_UA_001032.txt              URW: Discrediting the West, Diplomacy   \n",
       "3  EN_UA_001052.txt  URW: Blaming the war on others rather than the...   \n",
       "4  EN_UA_002668.txt                      URW: Speculating war outcomes   \n",
       "\n",
       "                                 Sub_Narratives_List  \\\n",
       "0  ['URW: Discrediting the West, Diplomacy: The E...   \n",
       "1  ['URW: Discrediting the West, Diplomacy: The E...   \n",
       "2  ['URW: Discrediting the West, Diplomacy: Diplo...   \n",
       "3  ['URW: Blaming the war on others rather than t...   \n",
       "4  ['URW: Speculating war outcomes: Other', 'URW:...   \n",
       "\n",
       "                                                Text  \\\n",
       "0  URW: Speculating war outcomes | Boris Johnson ...   \n",
       "1  URW: Discrediting the West, Diplomacy | Boris ...   \n",
       "2  URW: Discrediting the West, Diplomacy | Russia...   \n",
       "3  URW: Blaming the war on others rather than the...   \n",
       "4  URW: Speculating war outcomes | Putin may ABAN...   \n",
       "\n",
       "                             Filtered_Sub_Narratives  \\\n",
       "0             [URW: Speculating war outcomes: Other]   \n",
       "1  [URW: Discrediting the West, Diplomacy: The EU...   \n",
       "2  [URW: Discrediting the West, Diplomacy: Diplom...   \n",
       "3  [URW: Blaming the war on others rather than th...   \n",
       "4  [URW: Speculating war outcomes: Other, URW: Sp...   \n",
       "\n",
       "                                              Labels  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_multi_label(data, narratives):\n",
    "    label_vectors = []\n",
    "    for narratives_list in data['Filtered_Sub_Narratives']:\n",
    "        vector = [1 if narrative in narratives_list else 0 for narrative in narratives]\n",
    "        label_vectors.append(vector)\n",
    "    data['Labels'] = label_vectors\n",
    "    return data\n",
    "\n",
    "# Encode sub-level narratives\n",
    "df = preprocess_multi_label(df, sub_narratives)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: tensor([0.0141, 0.0197, 0.0049, 0.0493, 0.0123, 0.0247, 0.0090, 0.0123, 0.0197,\n",
      "        0.0082, 0.0099, 0.0493, 0.0197, 0.0329, 0.0070, 0.0082, 0.0123, 0.0058,\n",
      "        0.0164, 0.0076, 0.0329, 0.0037, 0.0986, 0.0986, 0.0090, 0.0099, 0.0493,\n",
      "        0.0164, 0.0123, 0.0986, 0.0123, 0.0038, 0.0070, 0.0141, 0.0247, 0.0076,\n",
      "        0.0247, 0.0493, 0.0070, 0.0329, 0.0329, 0.0110], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Compute class weights for multi-label classification\n",
    "labels = np.array(df['Labels'].tolist(), dtype=np.float32)\n",
    "\n",
    "label_sums = np.sum(labels, axis=0)  # Sum occurrences of each class\n",
    "class_weights = 1.0 / (label_sums + 1e-6)  # Avoid division by zero\n",
    "class_weights = class_weights / np.sum(class_weights)  # Normalize\n",
    "\n",
    "# Convert to torch tensor\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Print class weights\n",
    "print(\"Class Weights:\", class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Use Combined_Text as input (High-Level + Text)\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df['Text'], labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Convert to Hugging Face Dataset format\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "train_dataset = Dataset.from_dict({\"text\": train_texts.tolist(), \"labels\": train_labels.tolist()})\n",
    "test_dataset = Dataset.from_dict({\"text\": test_texts.tolist(), \"labels\": test_labels.tolist()})\n",
    "datasets = DatasetDict({\"train\": train_dataset, \"test\": test_dataset})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"roberta-base\", num_labels=labels.shape[1], problem_type=\"multi_label_classification\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from torch import nn\n",
    "\n",
    "class WeightedLossModel(nn.Module):\n",
    "    def __init__(self, base_model, class_weights):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.class_weights = class_weights\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss(weight=self.class_weights)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.loss_fn(logits, labels)\n",
    "\n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "# Wrap the model with class weights\n",
    "model = WeightedLossModel(model, class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16617d5f6bb4463286de8d791f7daad5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/211 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27e258f2e6d54f598b8700fa0d6a2319",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/53 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_datasets = datasets.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from transformers import EvalPrediction\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    # Apply sigmoid activation to model predictions (logits)\n",
    "    sigmoid_preds = torch.sigmoid(torch.tensor(p.predictions)).numpy()\n",
    "\n",
    "    # Convert probabilities to binary predictions using a threshold of 0.5\n",
    "    preds = (sigmoid_preds > 0.5).astype(int)\n",
    "\n",
    "    # Ground-truth labels\n",
    "    labels = p.label_ids\n",
    "\n",
    "    # Calculate weighted precision, recall, and F1-score\n",
    "    precision = precision_score(labels, preds, average=\"weighted\", zero_division=0)\n",
    "    recall = recall_score(labels, preds, average=\"weighted\", zero_division=0)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\", zero_division=0)\n",
    "\n",
    "    return {\"precision\": precision, \"recall\": recall, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\makan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=10,  # Reduced from 20 for efficiency\n",
    "    learning_rate=2e-5, \n",
    "    per_device_train_batch_size=6,\n",
    "    per_device_eval_batch_size=4,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1',\n",
    "    greater_is_better=True,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    save_total_limit=1,\n",
    "    seed=42,\n",
    "    # weight_decay=0.01,\n",
    "    # fp16=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\makan\\AppData\\Local\\Temp\\ipykernel_16440\\69762720.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44402971e6eb452aa98c2d461aa9caef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/360 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7706b163312b41ee9d852a4f4770d6da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.005536601413041353, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_runtime': 0.9817, 'eval_samples_per_second': 53.989, 'eval_steps_per_second': 14.261, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1cd4cabfb5f4694adddad315cc4ce92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0036604185588657856, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_runtime': 0.9871, 'eval_samples_per_second': 53.691, 'eval_steps_per_second': 14.183, 'epoch': 2.0}\n",
      "{'loss': 0.0065, 'grad_norm': 0.01106513012200594, 'learning_rate': 1.4444444444444446e-05, 'epoch': 2.78}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d175a62fccc4bb8a929f688fa572d3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0028155557811260223, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_runtime': 0.9833, 'eval_samples_per_second': 53.902, 'eval_steps_per_second': 14.238, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1df7879da1640fda0493280e5b13113",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0023728094529360533, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_runtime': 0.9877, 'eval_samples_per_second': 53.661, 'eval_steps_per_second': 14.175, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fd5190f44fa4190ae7692a1a5077c5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0021218028850853443, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_runtime': 0.9885, 'eval_samples_per_second': 53.618, 'eval_steps_per_second': 14.163, 'epoch': 5.0}\n",
      "{'loss': 0.0028, 'grad_norm': 0.005487695802003145, 'learning_rate': 8.888888888888888e-06, 'epoch': 5.56}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09eff945a36e4f52b351cc5ad2ae99f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0019692254718393087, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_runtime': 0.9898, 'eval_samples_per_second': 53.543, 'eval_steps_per_second': 14.144, 'epoch': 6.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed9ce60089ea45e6a1ecd0dd25bddc8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0018786722794175148, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_runtime': 0.9842, 'eval_samples_per_second': 53.851, 'eval_steps_per_second': 14.225, 'epoch': 7.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a029dfa97824d3faf91a7f2a5a6ddb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0018217733595520258, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_runtime': 0.9859, 'eval_samples_per_second': 53.757, 'eval_steps_per_second': 14.2, 'epoch': 8.0}\n",
      "{'loss': 0.0023, 'grad_norm': 0.004889245610684156, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.33}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6f8d1f2ca384d64a41b9323d192e9b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0017912110779434443, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_runtime': 0.9866, 'eval_samples_per_second': 53.721, 'eval_steps_per_second': 14.19, 'epoch': 9.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1a1152613cc41aa9beb3fae8ce9f8a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.001781550352461636, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_runtime': 0.9717, 'eval_samples_per_second': 54.541, 'eval_steps_per_second': 14.407, 'epoch': 10.0}\n",
      "{'train_runtime': 157.0425, 'train_samples_per_second': 13.436, 'train_steps_per_second': 2.292, 'train_loss': 0.0036137762996885513, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=360, training_loss=0.0036137762996885513, metrics={'train_runtime': 157.0425, 'train_samples_per_second': 13.436, 'train_steps_per_second': 2.292, 'total_flos': 0.0, 'train_loss': 0.0036137762996885513, 'epoch': 10.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azinja be bad ro kari nadashte bashid!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score, accuracy_score, recall_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unique_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m mlb \u001b[38;5;241m=\u001b[39m MultiLabelBinarizer(classes\u001b[38;5;241m=\u001b[39m\u001b[43munique_labels\u001b[49m)\n\u001b[0;32m      2\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncoded_Labels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m mlb\u001b[38;5;241m.\u001b[39mfit_transform(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFiltered_Sub_Narratives\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m      3\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'unique_labels' is not defined"
     ]
    }
   ],
   "source": [
    "mlb = MultiLabelBinarizer(classes=unique_labels)\n",
    "df[\"Encoded_Labels\"] = mlb.fit_transform(df[\"Filtered_Sub_Narratives\"]).tolist()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "df[\"Encoded_Labels\"] = df[\"Encoded_Labels\"].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "df = shuffle(df, random_state=42).reset_index(drop=True)\n",
    "\n",
    "train_size = int(0.8 * len(df))\n",
    "# Convert to list explicitly\n",
    "train_texts, train_labels = list(df[\"Text\"][:train_size]), list(df[\"Encoded_Labels\"][:train_size])\n",
    "test_texts, test_labels = list(df[\"Text\"][train_size:]), list(df[\"Encoded_Labels\"][train_size:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the tokenizer and model\n",
    "model_name = \"roberta-base\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(list(train_texts), truncation=True, padding=True, max_length=512)\n",
    "test_encodings = tokenizer(list(test_texts), truncation=True, padding=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NarrativeDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=42, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = NarrativeDataset(train_encodings, train_labels)\n",
    "test_dataset = NarrativeDataset(test_encodings, test_labels)\n",
    "\n",
    "# Define model\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=len(unique_labels))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\makan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=20,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    save_total_limit=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, accuracy_score, recall_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    preds = (logits > 0.5).astype(int)  # Convert logits to binary predictions for multi-label classification\n",
    "    labels = np.array(labels)  # Ensure labels are in array format\n",
    "\n",
    "    # Compute macro-averaged metrics\n",
    "    f1 = f1_score(labels, preds, average=\"macro\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    recall = recall_score(labels, preds, average=\"macro\")\n",
    "\n",
    "    return {\"f1\": f1, \"accuracy\": acc, \"recall\": recall}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "022e52144eeb4b1f8308406f4bbc960e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/540 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5459, 'grad_norm': 0.861284613609314, 'learning_rate': 4.9074074074074075e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3339, 'grad_norm': 0.5963953137397766, 'learning_rate': 4.814814814814815e-05, 'epoch': 0.74}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c4bc09b3db94575957faf117ce1251a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\makan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\makan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.23084905743598938, 'eval_f1': 0.0, 'eval_accuracy': 0.0, 'eval_recall': 0.0, 'eval_runtime': 0.9801, 'eval_samples_per_second': 54.077, 'eval_steps_per_second': 7.142, 'epoch': 1.0}\n",
      "{'loss': 0.2499, 'grad_norm': 0.45457470417022705, 'learning_rate': 4.722222222222222e-05, 'epoch': 1.11}\n",
      "{'loss': 0.1993, 'grad_norm': 0.34782975912094116, 'learning_rate': 4.62962962962963e-05, 'epoch': 1.48}\n",
      "{'loss': 0.178, 'grad_norm': 0.32758286595344543, 'learning_rate': 4.5370370370370374e-05, 'epoch': 1.85}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ec339a417c84dec994551ab77d26446",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\makan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\makan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.16114045679569244, 'eval_f1': 0.0, 'eval_accuracy': 0.0, 'eval_recall': 0.0, 'eval_runtime': 0.9701, 'eval_samples_per_second': 54.632, 'eval_steps_per_second': 7.216, 'epoch': 2.0}\n",
      "{'loss': 0.1549, 'grad_norm': 0.2748515009880066, 'learning_rate': 4.4444444444444447e-05, 'epoch': 2.22}\n",
      "{'loss': 0.1444, 'grad_norm': 0.2389254868030548, 'learning_rate': 4.351851851851852e-05, 'epoch': 2.59}\n",
      "{'loss': 0.1464, 'grad_norm': 0.26790982484817505, 'learning_rate': 4.259259259259259e-05, 'epoch': 2.96}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "769c01f6785240e4b261e103c1d15c8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\makan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\makan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1468096822500229, 'eval_f1': 0.0, 'eval_accuracy': 0.0, 'eval_recall': 0.0, 'eval_runtime': 0.9744, 'eval_samples_per_second': 54.395, 'eval_steps_per_second': 7.184, 'epoch': 3.0}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\makan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:2164\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2162\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2165\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2169\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\makan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:2527\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2521\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m   2522\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[0;32m   2524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2525\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2526\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m-> 2527\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   2528\u001b[0m ):\n\u001b[0;32m   2529\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2530\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[0;32m   2531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
