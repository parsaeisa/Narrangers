{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers import EvalPrediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_ID</th>\n",
       "      <th>High_Level_Narratives_List</th>\n",
       "      <th>Sub_Narratives_List</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EN_CC_100000.txt</td>\n",
       "      <td>['CC: Controversy about green technologies', '...</td>\n",
       "      <td>['CC: Criticism of institutions and authoritie...</td>\n",
       "      <td>Pentagon plans to serve LAB-GROWN MEAT to troo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EN_CC_100002.txt</td>\n",
       "      <td>['CC: Criticism of institutions and authoritie...</td>\n",
       "      <td>['CC: Criticism of institutions and authoritie...</td>\n",
       "      <td>Oxford Residents Mount Resistance Against the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EN_CC_100003.txt</td>\n",
       "      <td>['CC: Criticism of climate movement', 'CC: Cri...</td>\n",
       "      <td>['CC: Criticism of institutions and authoritie...</td>\n",
       "      <td>Fonda Heads To Canada For Oil Sands Protest, M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EN_CC_100004.txt</td>\n",
       "      <td>['CC: Criticism of institutions and authoritie...</td>\n",
       "      <td>['CC: Controversy about green technologies: Ot...</td>\n",
       "      <td>A Tesla Owner Just Exposed A Sick Secret About...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EN_CC_100005.txt</td>\n",
       "      <td>['CC: Criticism of climate movement']</td>\n",
       "      <td>['CC: Criticism of climate movement: Climate m...</td>\n",
       "      <td>Climate Crazies Fail in Attempt to Vandalize A...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Document_ID                         High_Level_Narratives_List  \\\n",
       "0  EN_CC_100000.txt  ['CC: Controversy about green technologies', '...   \n",
       "1  EN_CC_100002.txt  ['CC: Criticism of institutions and authoritie...   \n",
       "2  EN_CC_100003.txt  ['CC: Criticism of climate movement', 'CC: Cri...   \n",
       "3  EN_CC_100004.txt  ['CC: Criticism of institutions and authoritie...   \n",
       "4  EN_CC_100005.txt              ['CC: Criticism of climate movement']   \n",
       "\n",
       "                                 Sub_Narratives_List  \\\n",
       "0  ['CC: Criticism of institutions and authoritie...   \n",
       "1  ['CC: Criticism of institutions and authoritie...   \n",
       "2  ['CC: Criticism of institutions and authoritie...   \n",
       "3  ['CC: Controversy about green technologies: Ot...   \n",
       "4  ['CC: Criticism of climate movement: Climate m...   \n",
       "\n",
       "                                                Text  \n",
       "0  Pentagon plans to serve LAB-GROWN MEAT to troo...  \n",
       "1  Oxford Residents Mount Resistance Against the ...  \n",
       "2  Fonda Heads To Canada For Oil Sands Protest, M...  \n",
       "3  A Tesla Owner Just Exposed A Sick Secret About...  \n",
       "4  Climate Crazies Fail in Attempt to Vandalize A...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data_with_text.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_ID</th>\n",
       "      <th>High_Level_Narrative</th>\n",
       "      <th>Sub_Narratives_List</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EN_CC_100000.txt</td>\n",
       "      <td>CC: Controversy about green technologies</td>\n",
       "      <td>['CC: Criticism of institutions and authoritie...</td>\n",
       "      <td>Pentagon plans to serve LAB-GROWN MEAT to troo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EN_CC_100000.txt</td>\n",
       "      <td>CC: Criticism of institutions and authorities</td>\n",
       "      <td>['CC: Criticism of institutions and authoritie...</td>\n",
       "      <td>Pentagon plans to serve LAB-GROWN MEAT to troo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EN_CC_100000.txt</td>\n",
       "      <td>CC: Hidden plots by secret schemes of powerful...</td>\n",
       "      <td>['CC: Criticism of institutions and authoritie...</td>\n",
       "      <td>Pentagon plans to serve LAB-GROWN MEAT to troo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EN_CC_100002.txt</td>\n",
       "      <td>CC: Criticism of institutions and authorities</td>\n",
       "      <td>['CC: Criticism of institutions and authoritie...</td>\n",
       "      <td>Oxford Residents Mount Resistance Against the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EN_CC_100002.txt</td>\n",
       "      <td>CC: Hidden plots by secret schemes of powerful...</td>\n",
       "      <td>['CC: Criticism of institutions and authoritie...</td>\n",
       "      <td>Oxford Residents Mount Resistance Against the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Document_ID                               High_Level_Narrative  \\\n",
       "0  EN_CC_100000.txt           CC: Controversy about green technologies   \n",
       "0  EN_CC_100000.txt      CC: Criticism of institutions and authorities   \n",
       "0  EN_CC_100000.txt  CC: Hidden plots by secret schemes of powerful...   \n",
       "1  EN_CC_100002.txt      CC: Criticism of institutions and authorities   \n",
       "1  EN_CC_100002.txt  CC: Hidden plots by secret schemes of powerful...   \n",
       "\n",
       "                                 Sub_Narratives_List  \\\n",
       "0  ['CC: Criticism of institutions and authoritie...   \n",
       "0  ['CC: Criticism of institutions and authoritie...   \n",
       "0  ['CC: Criticism of institutions and authoritie...   \n",
       "1  ['CC: Criticism of institutions and authoritie...   \n",
       "1  ['CC: Criticism of institutions and authoritie...   \n",
       "\n",
       "                                                Text  \n",
       "0  Pentagon plans to serve LAB-GROWN MEAT to troo...  \n",
       "0  Pentagon plans to serve LAB-GROWN MEAT to troo...  \n",
       "0  Pentagon plans to serve LAB-GROWN MEAT to troo...  \n",
       "1  Oxford Residents Mount Resistance Against the ...  \n",
       "1  Oxford Residents Mount Resistance Against the ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the 'High_Level_Narratives_List' column from string to list\n",
    "df['High_Level_Narratives_List'] = df['High_Level_Narratives_List'].apply(lambda x: eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "# Explode the dataframe to have one row per High Level Narrative\n",
    "df = df.explode('High_Level_Narratives_List')\n",
    "\n",
    "# Rename the column for clarity\n",
    "df = df.rename(columns={\"High_Level_Narratives_List\": \"High_Level_Narrative\"})\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_ID</th>\n",
       "      <th>High_Level_Narrative</th>\n",
       "      <th>Sub_Narratives_List</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EN_CC_100000.txt</td>\n",
       "      <td>CC: Controversy about green technologies</td>\n",
       "      <td>['CC: Criticism of institutions and authoritie...</td>\n",
       "      <td>Pentagon plans to serve LAB-GROWN MEAT to troo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EN_CC_100000.txt</td>\n",
       "      <td>CC: Criticism of institutions and authorities</td>\n",
       "      <td>['CC: Criticism of institutions and authoritie...</td>\n",
       "      <td>Pentagon plans to serve LAB-GROWN MEAT to troo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EN_CC_100000.txt</td>\n",
       "      <td>CC: Hidden plots by secret schemes of powerful...</td>\n",
       "      <td>['CC: Criticism of institutions and authoritie...</td>\n",
       "      <td>Pentagon plans to serve LAB-GROWN MEAT to troo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EN_CC_100002.txt</td>\n",
       "      <td>CC: Criticism of institutions and authorities</td>\n",
       "      <td>['CC: Criticism of institutions and authoritie...</td>\n",
       "      <td>Oxford Residents Mount Resistance Against the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EN_CC_100002.txt</td>\n",
       "      <td>CC: Hidden plots by secret schemes of powerful...</td>\n",
       "      <td>['CC: Criticism of institutions and authoritie...</td>\n",
       "      <td>Oxford Residents Mount Resistance Against the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Document_ID                               High_Level_Narrative  \\\n",
       "0  EN_CC_100000.txt           CC: Controversy about green technologies   \n",
       "1  EN_CC_100000.txt      CC: Criticism of institutions and authorities   \n",
       "2  EN_CC_100000.txt  CC: Hidden plots by secret schemes of powerful...   \n",
       "3  EN_CC_100002.txt      CC: Criticism of institutions and authorities   \n",
       "4  EN_CC_100002.txt  CC: Hidden plots by secret schemes of powerful...   \n",
       "\n",
       "                                 Sub_Narratives_List  \\\n",
       "0  ['CC: Criticism of institutions and authoritie...   \n",
       "1  ['CC: Criticism of institutions and authoritie...   \n",
       "2  ['CC: Criticism of institutions and authoritie...   \n",
       "3  ['CC: Criticism of institutions and authoritie...   \n",
       "4  ['CC: Criticism of institutions and authoritie...   \n",
       "\n",
       "                                                Text  \n",
       "0  Pentagon plans to serve LAB-GROWN MEAT to troo...  \n",
       "1  Pentagon plans to serve LAB-GROWN MEAT to troo...  \n",
       "2  Pentagon plans to serve LAB-GROWN MEAT to troo...  \n",
       "3  Oxford Residents Mount Resistance Against the ...  \n",
       "4  Oxford Residents Mount Resistance Against the ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df[\"High_Level_Narrative\"].str.startswith(\"CC\")].reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 246 CC High Level Narratives after spliting multiple URW High Level Narratives in multiple rows\n"
     ]
    }
   ],
   "source": [
    "print(f'We have {len(df)} CC High Level Narratives after spliting multiple URW High Level Narratives in multiple rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_ID</th>\n",
       "      <th>High_Level_Narrative</th>\n",
       "      <th>Sub_Narratives_List</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EN_CC_100000.txt</td>\n",
       "      <td>CC: Controversy about green technologies</td>\n",
       "      <td>['CC: Criticism of institutions and authoritie...</td>\n",
       "      <td>CC: Controversy about green technologies | Pen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EN_CC_100000.txt</td>\n",
       "      <td>CC: Criticism of institutions and authorities</td>\n",
       "      <td>['CC: Criticism of institutions and authoritie...</td>\n",
       "      <td>CC: Criticism of institutions and authorities ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EN_CC_100000.txt</td>\n",
       "      <td>CC: Hidden plots by secret schemes of powerful...</td>\n",
       "      <td>['CC: Criticism of institutions and authoritie...</td>\n",
       "      <td>CC: Hidden plots by secret schemes of powerful...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EN_CC_100002.txt</td>\n",
       "      <td>CC: Criticism of institutions and authorities</td>\n",
       "      <td>['CC: Criticism of institutions and authoritie...</td>\n",
       "      <td>CC: Criticism of institutions and authorities ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EN_CC_100002.txt</td>\n",
       "      <td>CC: Hidden plots by secret schemes of powerful...</td>\n",
       "      <td>['CC: Criticism of institutions and authoritie...</td>\n",
       "      <td>CC: Hidden plots by secret schemes of powerful...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Document_ID                               High_Level_Narrative  \\\n",
       "0  EN_CC_100000.txt           CC: Controversy about green technologies   \n",
       "1  EN_CC_100000.txt      CC: Criticism of institutions and authorities   \n",
       "2  EN_CC_100000.txt  CC: Hidden plots by secret schemes of powerful...   \n",
       "3  EN_CC_100002.txt      CC: Criticism of institutions and authorities   \n",
       "4  EN_CC_100002.txt  CC: Hidden plots by secret schemes of powerful...   \n",
       "\n",
       "                                 Sub_Narratives_List  \\\n",
       "0  ['CC: Criticism of institutions and authoritie...   \n",
       "1  ['CC: Criticism of institutions and authoritie...   \n",
       "2  ['CC: Criticism of institutions and authoritie...   \n",
       "3  ['CC: Criticism of institutions and authoritie...   \n",
       "4  ['CC: Criticism of institutions and authoritie...   \n",
       "\n",
       "                                                Text  \n",
       "0  CC: Controversy about green technologies | Pen...  \n",
       "1  CC: Criticism of institutions and authorities ...  \n",
       "2  CC: Hidden plots by secret schemes of powerful...  \n",
       "3  CC: Criticism of institutions and authorities ...  \n",
       "4  CC: Hidden plots by secret schemes of powerful...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#in here we will add high level narrative to text with a | seperating high level narrative and text\n",
    "df[\"Text\"] = df[\"High_Level_Narrative\"] + \" | \" + df[\"Text\"]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [CC: Controversy about green technologies: Other]\n",
       "1      [CC: Criticism of institutions and authorities...\n",
       "2      [CC: Hidden plots by secret schemes of powerfu...\n",
       "3      [CC: Criticism of institutions and authorities...\n",
       "4      [CC: Hidden plots by secret schemes of powerfu...\n",
       "                             ...                        \n",
       "241    [CC: Hidden plots by secret schemes of powerfu...\n",
       "242    [CC: Amplifying Climate Fears: Amplifying exis...\n",
       "243    [CC: Criticism of institutions and authorities...\n",
       "244    [CC: Criticism of institutions and authorities...\n",
       "245    [CC: Criticism of climate policies: Climate po...\n",
       "Name: Filtered_Sub_Narratives, Length: 246, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to filter sub-narratives that match the high-level narrative\n",
    "def filter_sub_narratives(row):\n",
    "    high_level = row[\"High_Level_Narrative\"].strip()\n",
    "    sub_narratives = eval(row[\"Sub_Narratives_List\"])  # Convert string representation of list to actual list\n",
    "    \n",
    "    # Keep only sub-narratives that start with the high-level narrative\n",
    "    filtered = [sub for sub in sub_narratives if sub.startswith(high_level)]\n",
    "    return filtered\n",
    "\n",
    "# Apply filtering function\n",
    "df[\"Filtered_Sub_Narratives\"] = df.apply(filter_sub_narratives, axis=1)\n",
    "df['Filtered_Sub_Narratives']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all unique sub-narratives\n",
    "sub_narratives = list(set(label for sublist in df[\"Filtered_Sub_Narratives\"] for label in sublist))\n",
    "len(sub_narratives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_ID</th>\n",
       "      <th>High_Level_Narrative</th>\n",
       "      <th>Sub_Narratives_List</th>\n",
       "      <th>Text</th>\n",
       "      <th>Filtered_Sub_Narratives</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EN_CC_100000.txt</td>\n",
       "      <td>CC: Controversy about green technologies</td>\n",
       "      <td>['CC: Criticism of institutions and authoritie...</td>\n",
       "      <td>CC: Controversy about green technologies | Pen...</td>\n",
       "      <td>[CC: Controversy about green technologies: Other]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EN_CC_100000.txt</td>\n",
       "      <td>CC: Criticism of institutions and authorities</td>\n",
       "      <td>['CC: Criticism of institutions and authoritie...</td>\n",
       "      <td>CC: Criticism of institutions and authorities ...</td>\n",
       "      <td>[CC: Criticism of institutions and authorities...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EN_CC_100000.txt</td>\n",
       "      <td>CC: Hidden plots by secret schemes of powerful...</td>\n",
       "      <td>['CC: Criticism of institutions and authoritie...</td>\n",
       "      <td>CC: Hidden plots by secret schemes of powerful...</td>\n",
       "      <td>[CC: Hidden plots by secret schemes of powerfu...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EN_CC_100002.txt</td>\n",
       "      <td>CC: Criticism of institutions and authorities</td>\n",
       "      <td>['CC: Criticism of institutions and authoritie...</td>\n",
       "      <td>CC: Criticism of institutions and authorities ...</td>\n",
       "      <td>[CC: Criticism of institutions and authorities...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EN_CC_100002.txt</td>\n",
       "      <td>CC: Hidden plots by secret schemes of powerful...</td>\n",
       "      <td>['CC: Criticism of institutions and authoritie...</td>\n",
       "      <td>CC: Hidden plots by secret schemes of powerful...</td>\n",
       "      <td>[CC: Hidden plots by secret schemes of powerfu...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Document_ID                               High_Level_Narrative  \\\n",
       "0  EN_CC_100000.txt           CC: Controversy about green technologies   \n",
       "1  EN_CC_100000.txt      CC: Criticism of institutions and authorities   \n",
       "2  EN_CC_100000.txt  CC: Hidden plots by secret schemes of powerful...   \n",
       "3  EN_CC_100002.txt      CC: Criticism of institutions and authorities   \n",
       "4  EN_CC_100002.txt  CC: Hidden plots by secret schemes of powerful...   \n",
       "\n",
       "                                 Sub_Narratives_List  \\\n",
       "0  ['CC: Criticism of institutions and authoritie...   \n",
       "1  ['CC: Criticism of institutions and authoritie...   \n",
       "2  ['CC: Criticism of institutions and authoritie...   \n",
       "3  ['CC: Criticism of institutions and authoritie...   \n",
       "4  ['CC: Criticism of institutions and authoritie...   \n",
       "\n",
       "                                                Text  \\\n",
       "0  CC: Controversy about green technologies | Pen...   \n",
       "1  CC: Criticism of institutions and authorities ...   \n",
       "2  CC: Hidden plots by secret schemes of powerful...   \n",
       "3  CC: Criticism of institutions and authorities ...   \n",
       "4  CC: Hidden plots by secret schemes of powerful...   \n",
       "\n",
       "                             Filtered_Sub_Narratives  \\\n",
       "0  [CC: Controversy about green technologies: Other]   \n",
       "1  [CC: Criticism of institutions and authorities...   \n",
       "2  [CC: Hidden plots by secret schemes of powerfu...   \n",
       "3  [CC: Criticism of institutions and authorities...   \n",
       "4  [CC: Hidden plots by secret schemes of powerfu...   \n",
       "\n",
       "                                              Labels  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_multi_label(data, narratives):\n",
    "    label_vectors = []\n",
    "    for narratives_list in data['Filtered_Sub_Narratives']:\n",
    "        vector = [1 if narrative in narratives_list else 0 for narrative in narratives]\n",
    "        label_vectors.append(vector)\n",
    "    data['Labels'] = label_vectors\n",
    "    return data\n",
    "\n",
    "# Encode sub-level narratives\n",
    "df = preprocess_multi_label(df, sub_narratives)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_rare_classes(data, labels, min_samples=15):\n",
    "    label_sums = np.sum(labels, axis=0)\n",
    "    rare_classes = np.where(label_sums < min_samples)[0]\n",
    "\n",
    "    for rare_class in rare_classes:\n",
    "        rare_indices = [i for i, lbl in enumerate(labels) if lbl[rare_class] == 1]\n",
    "        if len(rare_indices) > 0:\n",
    "            duplicate_data = data.iloc[rare_indices]\n",
    "            data = pd.concat([data] + [duplicate_data] * (min_samples - len(rare_indices)), ignore_index=True)\n",
    "            labels = np.vstack([labels] + [labels[rare_indices]] * (min_samples - len(rare_indices)))\n",
    "    return data, labels\n",
    "\n",
    "# Apply rare class handling\n",
    "labels = np.array(df['Labels'].tolist(), dtype=np.float32)\n",
    "df, labels = handle_rare_classes(df, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Use Combined_Text as input (High-Level + Text)\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df['Text'], labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Convert to Hugging Face Dataset format\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "train_dataset = Dataset.from_dict({\"text\": train_texts.tolist(), \"labels\": train_labels.tolist()})\n",
    "test_dataset = Dataset.from_dict({\"text\": test_texts.tolist(), \"labels\": test_labels.tolist()})\n",
    "datasets = DatasetDict({\"train\": train_dataset, \"test\": test_dataset})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"roberta-base\", num_labels=labels.shape[1], problem_type=\"multi_label_classification\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79024adcad0144b7a57b1f4cc78979b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/876 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "214ae60035094216b8c7ccfd38fa8db5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/220 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_datasets = datasets.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from transformers import EvalPrediction\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    # Apply sigmoid activation to model predictions (logits)\n",
    "    sigmoid_preds = torch.sigmoid(torch.tensor(p.predictions)).numpy()\n",
    "\n",
    "    # Convert probabilities to binary predictions using a threshold of 0.5\n",
    "    preds = (sigmoid_preds > 0.5).astype(int)\n",
    "\n",
    "    # Ground-truth labels\n",
    "    labels = p.label_ids\n",
    "\n",
    "    # Calculate weighted precision, recall, and F1-score\n",
    "    precision = precision_score(labels, preds, average=\"weighted\", zero_division=0)\n",
    "    recall = recall_score(labels, preds, average=\"weighted\", zero_division=0)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\", zero_division=0)\n",
    "\n",
    "    return {\"precision\": precision, \"recall\": recall, \"f1\": f1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\makan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=10,  # Reduced from 20 for efficiency\n",
    "    learning_rate=2e-5, \n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1',\n",
    "    greater_is_better=True,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    save_total_limit=1,\n",
    "    seed=42,\n",
    "    weight_decay=0.01,\n",
    "    # fp16=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\makan\\AppData\\Local\\Temp\\ipykernel_21744\\69762720.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3153412d461a499893c241af9eaf8727",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2190 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3461, 'grad_norm': 0.4525870680809021, 'learning_rate': 1.9086757990867582e-05, 'epoch': 0.46}\n",
      "{'loss': 0.1979, 'grad_norm': 0.5238326191902161, 'learning_rate': 1.8173515981735163e-05, 'epoch': 0.91}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f84fe64f0bae4ef6827cb47970ab6674",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1741182506084442, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_runtime': 3.9727, 'eval_samples_per_second': 55.378, 'eval_steps_per_second': 13.845, 'epoch': 1.0}\n",
      "{'loss': 0.1736, 'grad_norm': 0.42370036244392395, 'learning_rate': 1.726027397260274e-05, 'epoch': 1.37}\n",
      "{'loss': 0.1591, 'grad_norm': 0.38125738501548767, 'learning_rate': 1.634703196347032e-05, 'epoch': 1.83}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4156966cb2714179b78da724a73e77dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.13422809541225433, 'eval_precision': 0.2802056555269923, 'eval_recall': 0.16966580976863754, 'eval_f1': 0.20609141976751233, 'eval_runtime': 3.972, 'eval_samples_per_second': 55.388, 'eval_steps_per_second': 13.847, 'epoch': 2.0}\n",
      "{'loss': 0.1383, 'grad_norm': 0.4957568943500519, 'learning_rate': 1.54337899543379e-05, 'epoch': 2.28}\n",
      "{'loss': 0.1223, 'grad_norm': 0.45921605825424194, 'learning_rate': 1.4520547945205482e-05, 'epoch': 2.74}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b51df7af4f448e7ad7056ee2f74a706",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.10351112484931946, 'eval_precision': 0.4630616554204889, 'eval_recall': 0.33161953727506427, 'eval_f1': 0.36882968593248794, 'eval_runtime': 3.9699, 'eval_samples_per_second': 55.417, 'eval_steps_per_second': 13.854, 'epoch': 3.0}\n",
      "{'loss': 0.1099, 'grad_norm': 0.3648008704185486, 'learning_rate': 1.360730593607306e-05, 'epoch': 3.2}\n",
      "{'loss': 0.0993, 'grad_norm': 0.3995565176010132, 'learning_rate': 1.2694063926940641e-05, 'epoch': 3.65}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd911ef83b524255a208c3b186dc3d47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.08436321467161179, 'eval_precision': 0.5993612214691906, 'eval_recall': 0.3856041131105398, 'eval_f1': 0.4440775812408762, 'eval_runtime': 3.9754, 'eval_samples_per_second': 55.34, 'eval_steps_per_second': 13.835, 'epoch': 4.0}\n",
      "{'loss': 0.0935, 'grad_norm': 0.37890681624412537, 'learning_rate': 1.178082191780822e-05, 'epoch': 4.11}\n",
      "{'loss': 0.0855, 'grad_norm': 0.3296149671077728, 'learning_rate': 1.08675799086758e-05, 'epoch': 4.57}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ca6bcc8d1dd4b6ebba5cec2291995a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.07308944314718246, 'eval_precision': 0.7920610880122448, 'eval_recall': 0.6246786632390745, 'eval_f1': 0.6817593994591403, 'eval_runtime': 3.9699, 'eval_samples_per_second': 55.417, 'eval_steps_per_second': 13.854, 'epoch': 5.0}\n",
      "{'loss': 0.0798, 'grad_norm': 0.4065277874469757, 'learning_rate': 9.95433789954338e-06, 'epoch': 5.02}\n",
      "{'loss': 0.0754, 'grad_norm': 0.23864445090293884, 'learning_rate': 9.04109589041096e-06, 'epoch': 5.48}\n",
      "{'loss': 0.0717, 'grad_norm': 0.3669087290763855, 'learning_rate': 8.127853881278539e-06, 'epoch': 5.94}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c768bc0635544458198a83770bd31c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.06585733592510223, 'eval_precision': 0.8264034951052945, 'eval_recall': 0.7634961439588689, 'eval_f1': 0.7790605191843858, 'eval_runtime': 3.9707, 'eval_samples_per_second': 55.406, 'eval_steps_per_second': 13.851, 'epoch': 6.0}\n",
      "{'loss': 0.0671, 'grad_norm': 0.28745871782302856, 'learning_rate': 7.214611872146119e-06, 'epoch': 6.39}\n",
      "{'loss': 0.0659, 'grad_norm': 0.3588755428791046, 'learning_rate': 6.301369863013699e-06, 'epoch': 6.85}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f552498d9fa54970b03688d4a59db4ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.059628330171108246, 'eval_precision': 0.8628709978967047, 'eval_recall': 0.7377892030848329, 'eval_f1': 0.7785816807689074, 'eval_runtime': 3.9766, 'eval_samples_per_second': 55.323, 'eval_steps_per_second': 13.831, 'epoch': 7.0}\n",
      "{'loss': 0.0625, 'grad_norm': 0.2712402939796448, 'learning_rate': 5.388127853881279e-06, 'epoch': 7.31}\n",
      "{'loss': 0.0599, 'grad_norm': 0.1956748217344284, 'learning_rate': 4.4748858447488585e-06, 'epoch': 7.76}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4412793d0e6d4df583497bf0b224c1e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.05616539716720581, 'eval_precision': 0.9173880746502855, 'eval_recall': 0.8329048843187661, 'eval_f1': 0.8566576484711694, 'eval_runtime': 3.9768, 'eval_samples_per_second': 55.321, 'eval_steps_per_second': 13.83, 'epoch': 8.0}\n",
      "{'loss': 0.059, 'grad_norm': 0.25419795513153076, 'learning_rate': 3.5616438356164386e-06, 'epoch': 8.22}\n",
      "{'loss': 0.058, 'grad_norm': 0.3610170781612396, 'learning_rate': 2.6484018264840183e-06, 'epoch': 8.68}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a3fb2ec76b34b308c7a844e4da23005",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.05413048341870308, 'eval_precision': 0.9145294294394551, 'eval_recall': 0.8431876606683805, 'eval_f1': 0.8604948604552136, 'eval_runtime': 3.973, 'eval_samples_per_second': 55.374, 'eval_steps_per_second': 13.844, 'epoch': 9.0}\n",
      "{'loss': 0.0569, 'grad_norm': 0.2564339339733124, 'learning_rate': 1.7351598173515982e-06, 'epoch': 9.13}\n",
      "{'loss': 0.0555, 'grad_norm': 0.28643083572387695, 'learning_rate': 8.219178082191781e-07, 'epoch': 9.59}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2b923dc42a3403487838d7ffc2e2afa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.05331533029675484, 'eval_precision': 0.9141162821754081, 'eval_recall': 0.8508997429305912, 'eval_f1': 0.8699059040716103, 'eval_runtime': 4.0231, 'eval_samples_per_second': 54.684, 'eval_steps_per_second': 13.671, 'epoch': 10.0}\n",
      "{'train_runtime': 615.3492, 'train_samples_per_second': 14.236, 'train_steps_per_second': 3.559, 'train_loss': 0.10445904927710964, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2190, training_loss=0.10445904927710964, metrics={'train_runtime': 615.3492, 'train_samples_per_second': 14.236, 'train_steps_per_second': 3.559, 'total_flos': 2305659924357120.0, 'train_loss': 0.10445904927710964, 'epoch': 10.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azinja be bad ro kari nadashte bashid!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score, accuracy_score, recall_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_ID</th>\n",
       "      <th>High_Level_Narrative</th>\n",
       "      <th>Sub_Narratives_List</th>\n",
       "      <th>Text</th>\n",
       "      <th>Filtered_Sub_Narratives</th>\n",
       "      <th>Encoded_Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EN_UA_000923.txt</td>\n",
       "      <td>URW: Speculating war outcomes</td>\n",
       "      <td>['URW: Discrediting the West, Diplomacy: The E...</td>\n",
       "      <td>URW: Speculating war outcomes | Boris Johnson ...</td>\n",
       "      <td>[URW: Speculating war outcomes: Other]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EN_UA_000923.txt</td>\n",
       "      <td>URW: Discrediting the West, Diplomacy</td>\n",
       "      <td>['URW: Discrediting the West, Diplomacy: The E...</td>\n",
       "      <td>URW: Discrediting the West, Diplomacy | Boris ...</td>\n",
       "      <td>[URW: Discrediting the West, Diplomacy: The EU...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EN_UA_001032.txt</td>\n",
       "      <td>URW: Discrediting the West, Diplomacy</td>\n",
       "      <td>['URW: Discrediting the West, Diplomacy: Diplo...</td>\n",
       "      <td>URW: Discrediting the West, Diplomacy | Russia...</td>\n",
       "      <td>[URW: Discrediting the West, Diplomacy: Diplom...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EN_UA_001052.txt</td>\n",
       "      <td>URW: Blaming the war on others rather than the...</td>\n",
       "      <td>['URW: Blaming the war on others rather than t...</td>\n",
       "      <td>URW: Blaming the war on others rather than the...</td>\n",
       "      <td>[URW: Blaming the war on others rather than th...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EN_UA_002668.txt</td>\n",
       "      <td>URW: Speculating war outcomes</td>\n",
       "      <td>['URW: Speculating war outcomes: Other', 'URW:...</td>\n",
       "      <td>URW: Speculating war outcomes | Putin may ABAN...</td>\n",
       "      <td>[URW: Speculating war outcomes: Other, URW: Sp...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Document_ID                               High_Level_Narrative  \\\n",
       "0  EN_UA_000923.txt                      URW: Speculating war outcomes   \n",
       "1  EN_UA_000923.txt              URW: Discrediting the West, Diplomacy   \n",
       "2  EN_UA_001032.txt              URW: Discrediting the West, Diplomacy   \n",
       "3  EN_UA_001052.txt  URW: Blaming the war on others rather than the...   \n",
       "4  EN_UA_002668.txt                      URW: Speculating war outcomes   \n",
       "\n",
       "                                 Sub_Narratives_List  \\\n",
       "0  ['URW: Discrediting the West, Diplomacy: The E...   \n",
       "1  ['URW: Discrediting the West, Diplomacy: The E...   \n",
       "2  ['URW: Discrediting the West, Diplomacy: Diplo...   \n",
       "3  ['URW: Blaming the war on others rather than t...   \n",
       "4  ['URW: Speculating war outcomes: Other', 'URW:...   \n",
       "\n",
       "                                                Text  \\\n",
       "0  URW: Speculating war outcomes | Boris Johnson ...   \n",
       "1  URW: Discrediting the West, Diplomacy | Boris ...   \n",
       "2  URW: Discrediting the West, Diplomacy | Russia...   \n",
       "3  URW: Blaming the war on others rather than the...   \n",
       "4  URW: Speculating war outcomes | Putin may ABAN...   \n",
       "\n",
       "                             Filtered_Sub_Narratives  \\\n",
       "0             [URW: Speculating war outcomes: Other]   \n",
       "1  [URW: Discrediting the West, Diplomacy: The EU...   \n",
       "2  [URW: Discrediting the West, Diplomacy: Diplom...   \n",
       "3  [URW: Blaming the war on others rather than th...   \n",
       "4  [URW: Speculating war outcomes: Other, URW: Sp...   \n",
       "\n",
       "                                      Encoded_Labels  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlb = MultiLabelBinarizer(classes=unique_labels)\n",
    "df[\"Encoded_Labels\"] = mlb.fit_transform(df[\"Filtered_Sub_Narratives\"]).tolist()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "df[\"Encoded_Labels\"] = df[\"Encoded_Labels\"].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "df = shuffle(df, random_state=42).reset_index(drop=True)\n",
    "\n",
    "train_size = int(0.8 * len(df))\n",
    "# Convert to list explicitly\n",
    "train_texts, train_labels = list(df[\"Text\"][:train_size]), list(df[\"Encoded_Labels\"][:train_size])\n",
    "test_texts, test_labels = list(df[\"Text\"][train_size:]), list(df[\"Encoded_Labels\"][train_size:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the tokenizer and model\n",
    "model_name = \"roberta-base\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(list(train_texts), truncation=True, padding=True, max_length=512)\n",
    "test_encodings = tokenizer(list(test_texts), truncation=True, padding=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NarrativeDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=42, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = NarrativeDataset(train_encodings, train_labels)\n",
    "test_dataset = NarrativeDataset(test_encodings, test_labels)\n",
    "\n",
    "# Define model\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=len(unique_labels))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\makan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=20,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    save_total_limit=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, accuracy_score, recall_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    preds = (logits > 0.5).astype(int)  # Convert logits to binary predictions for multi-label classification\n",
    "    labels = np.array(labels)  # Ensure labels are in array format\n",
    "\n",
    "    # Compute macro-averaged metrics\n",
    "    f1 = f1_score(labels, preds, average=\"macro\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    recall = recall_score(labels, preds, average=\"macro\")\n",
    "\n",
    "    return {\"f1\": f1, \"accuracy\": acc, \"recall\": recall}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "022e52144eeb4b1f8308406f4bbc960e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/540 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5459, 'grad_norm': 0.861284613609314, 'learning_rate': 4.9074074074074075e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3339, 'grad_norm': 0.5963953137397766, 'learning_rate': 4.814814814814815e-05, 'epoch': 0.74}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c4bc09b3db94575957faf117ce1251a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\makan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\makan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.23084905743598938, 'eval_f1': 0.0, 'eval_accuracy': 0.0, 'eval_recall': 0.0, 'eval_runtime': 0.9801, 'eval_samples_per_second': 54.077, 'eval_steps_per_second': 7.142, 'epoch': 1.0}\n",
      "{'loss': 0.2499, 'grad_norm': 0.45457470417022705, 'learning_rate': 4.722222222222222e-05, 'epoch': 1.11}\n",
      "{'loss': 0.1993, 'grad_norm': 0.34782975912094116, 'learning_rate': 4.62962962962963e-05, 'epoch': 1.48}\n",
      "{'loss': 0.178, 'grad_norm': 0.32758286595344543, 'learning_rate': 4.5370370370370374e-05, 'epoch': 1.85}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ec339a417c84dec994551ab77d26446",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\makan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\makan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.16114045679569244, 'eval_f1': 0.0, 'eval_accuracy': 0.0, 'eval_recall': 0.0, 'eval_runtime': 0.9701, 'eval_samples_per_second': 54.632, 'eval_steps_per_second': 7.216, 'epoch': 2.0}\n",
      "{'loss': 0.1549, 'grad_norm': 0.2748515009880066, 'learning_rate': 4.4444444444444447e-05, 'epoch': 2.22}\n",
      "{'loss': 0.1444, 'grad_norm': 0.2389254868030548, 'learning_rate': 4.351851851851852e-05, 'epoch': 2.59}\n",
      "{'loss': 0.1464, 'grad_norm': 0.26790982484817505, 'learning_rate': 4.259259259259259e-05, 'epoch': 2.96}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "769c01f6785240e4b261e103c1d15c8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\makan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\makan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1468096822500229, 'eval_f1': 0.0, 'eval_accuracy': 0.0, 'eval_recall': 0.0, 'eval_runtime': 0.9744, 'eval_samples_per_second': 54.395, 'eval_steps_per_second': 7.184, 'epoch': 3.0}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\makan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:2164\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2162\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2165\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2169\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\makan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:2527\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2521\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m   2522\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[0;32m   2524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2525\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2526\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m-> 2527\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   2528\u001b[0m ):\n\u001b[0;32m   2529\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2530\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[0;32m   2531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
