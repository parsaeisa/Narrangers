{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers import EvalPrediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_ID</th>\n",
       "      <th>High_Level_Narratives_List</th>\n",
       "      <th>Sub_Narratives_List</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EN_CC_100000.txt</td>\n",
       "      <td>['CC: Controversy about green technologies', '...</td>\n",
       "      <td>['CC: Criticism of institutions and authoritie...</td>\n",
       "      <td>Pentagon plans to serve LAB-GROWN MEAT to troo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EN_CC_100002.txt</td>\n",
       "      <td>['CC: Criticism of institutions and authoritie...</td>\n",
       "      <td>['CC: Criticism of institutions and authoritie...</td>\n",
       "      <td>Oxford Residents Mount Resistance Against the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EN_CC_100003.txt</td>\n",
       "      <td>['CC: Criticism of climate movement', 'CC: Cri...</td>\n",
       "      <td>['CC: Criticism of institutions and authoritie...</td>\n",
       "      <td>Fonda Heads To Canada For Oil Sands Protest, M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EN_CC_100004.txt</td>\n",
       "      <td>['CC: Criticism of institutions and authoritie...</td>\n",
       "      <td>['CC: Controversy about green technologies: Ot...</td>\n",
       "      <td>A Tesla Owner Just Exposed A Sick Secret About...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EN_CC_100005.txt</td>\n",
       "      <td>['CC: Criticism of climate movement']</td>\n",
       "      <td>['CC: Criticism of climate movement: Climate m...</td>\n",
       "      <td>Climate Crazies Fail in Attempt to Vandalize A...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Document_ID                         High_Level_Narratives_List  \\\n",
       "0  EN_CC_100000.txt  ['CC: Controversy about green technologies', '...   \n",
       "1  EN_CC_100002.txt  ['CC: Criticism of institutions and authoritie...   \n",
       "2  EN_CC_100003.txt  ['CC: Criticism of climate movement', 'CC: Cri...   \n",
       "3  EN_CC_100004.txt  ['CC: Criticism of institutions and authoritie...   \n",
       "4  EN_CC_100005.txt              ['CC: Criticism of climate movement']   \n",
       "\n",
       "                                 Sub_Narratives_List  \\\n",
       "0  ['CC: Criticism of institutions and authoritie...   \n",
       "1  ['CC: Criticism of institutions and authoritie...   \n",
       "2  ['CC: Criticism of institutions and authoritie...   \n",
       "3  ['CC: Controversy about green technologies: Ot...   \n",
       "4  ['CC: Criticism of climate movement: Climate m...   \n",
       "\n",
       "                                                Text  \n",
       "0  Pentagon plans to serve LAB-GROWN MEAT to troo...  \n",
       "1  Oxford Residents Mount Resistance Against the ...  \n",
       "2  Fonda Heads To Canada For Oil Sands Protest, M...  \n",
       "3  A Tesla Owner Just Exposed A Sick Secret About...  \n",
       "4  Climate Crazies Fail in Attempt to Vandalize A...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data_with_text.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_ID</th>\n",
       "      <th>High_Level_Narrative</th>\n",
       "      <th>Sub_Narratives_List</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EN_CC_100000.txt</td>\n",
       "      <td>CC: Controversy about green technologies</td>\n",
       "      <td>['CC: Criticism of institutions and authoritie...</td>\n",
       "      <td>Pentagon plans to serve LAB-GROWN MEAT to troo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EN_CC_100000.txt</td>\n",
       "      <td>CC: Criticism of institutions and authorities</td>\n",
       "      <td>['CC: Criticism of institutions and authoritie...</td>\n",
       "      <td>Pentagon plans to serve LAB-GROWN MEAT to troo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EN_CC_100000.txt</td>\n",
       "      <td>CC: Hidden plots by secret schemes of powerful...</td>\n",
       "      <td>['CC: Criticism of institutions and authoritie...</td>\n",
       "      <td>Pentagon plans to serve LAB-GROWN MEAT to troo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EN_CC_100002.txt</td>\n",
       "      <td>CC: Criticism of institutions and authorities</td>\n",
       "      <td>['CC: Criticism of institutions and authoritie...</td>\n",
       "      <td>Oxford Residents Mount Resistance Against the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EN_CC_100002.txt</td>\n",
       "      <td>CC: Hidden plots by secret schemes of powerful...</td>\n",
       "      <td>['CC: Criticism of institutions and authoritie...</td>\n",
       "      <td>Oxford Residents Mount Resistance Against the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Document_ID                               High_Level_Narrative  \\\n",
       "0  EN_CC_100000.txt           CC: Controversy about green technologies   \n",
       "0  EN_CC_100000.txt      CC: Criticism of institutions and authorities   \n",
       "0  EN_CC_100000.txt  CC: Hidden plots by secret schemes of powerful...   \n",
       "1  EN_CC_100002.txt      CC: Criticism of institutions and authorities   \n",
       "1  EN_CC_100002.txt  CC: Hidden plots by secret schemes of powerful...   \n",
       "\n",
       "                                 Sub_Narratives_List  \\\n",
       "0  ['CC: Criticism of institutions and authoritie...   \n",
       "0  ['CC: Criticism of institutions and authoritie...   \n",
       "0  ['CC: Criticism of institutions and authoritie...   \n",
       "1  ['CC: Criticism of institutions and authoritie...   \n",
       "1  ['CC: Criticism of institutions and authoritie...   \n",
       "\n",
       "                                                Text  \n",
       "0  Pentagon plans to serve LAB-GROWN MEAT to troo...  \n",
       "0  Pentagon plans to serve LAB-GROWN MEAT to troo...  \n",
       "0  Pentagon plans to serve LAB-GROWN MEAT to troo...  \n",
       "1  Oxford Residents Mount Resistance Against the ...  \n",
       "1  Oxford Residents Mount Resistance Against the ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the 'High_Level_Narratives_List' column from string to list\n",
    "df['High_Level_Narratives_List'] = df['High_Level_Narratives_List'].apply(lambda x: eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "# Explode the dataframe to have one row per High Level Narrative\n",
    "df = df.explode('High_Level_Narratives_List')\n",
    "\n",
    "# Rename the column for clarity\n",
    "df = df.rename(columns={\"High_Level_Narratives_List\": \"High_Level_Narrative\"})\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_ID</th>\n",
       "      <th>High_Level_Narrative</th>\n",
       "      <th>Sub_Narratives_List</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EN_UA_000923.txt</td>\n",
       "      <td>URW: Speculating war outcomes</td>\n",
       "      <td>['URW: Discrediting the West, Diplomacy: The E...</td>\n",
       "      <td>Boris Johnson demands Putin ‘steps back from t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EN_UA_000923.txt</td>\n",
       "      <td>URW: Discrediting the West, Diplomacy</td>\n",
       "      <td>['URW: Discrediting the West, Diplomacy: The E...</td>\n",
       "      <td>Boris Johnson demands Putin ‘steps back from t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EN_UA_001032.txt</td>\n",
       "      <td>URW: Discrediting the West, Diplomacy</td>\n",
       "      <td>['URW: Discrediting the West, Diplomacy: Diplo...</td>\n",
       "      <td>Russia-Ukraine war map: Where are Russian troo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EN_UA_001052.txt</td>\n",
       "      <td>URW: Blaming the war on others rather than the...</td>\n",
       "      <td>['URW: Blaming the war on others rather than t...</td>\n",
       "      <td>NATO ‘Cautiously Optimistic’ Amid Reports of R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EN_UA_002668.txt</td>\n",
       "      <td>URW: Speculating war outcomes</td>\n",
       "      <td>['URW: Speculating war outcomes: Other', 'URW:...</td>\n",
       "      <td>Putin may ABANDON siege of Kyiv and try to bli...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Document_ID                               High_Level_Narrative  \\\n",
       "0  EN_UA_000923.txt                      URW: Speculating war outcomes   \n",
       "1  EN_UA_000923.txt              URW: Discrediting the West, Diplomacy   \n",
       "2  EN_UA_001032.txt              URW: Discrediting the West, Diplomacy   \n",
       "3  EN_UA_001052.txt  URW: Blaming the war on others rather than the...   \n",
       "4  EN_UA_002668.txt                      URW: Speculating war outcomes   \n",
       "\n",
       "                                 Sub_Narratives_List  \\\n",
       "0  ['URW: Discrediting the West, Diplomacy: The E...   \n",
       "1  ['URW: Discrediting the West, Diplomacy: The E...   \n",
       "2  ['URW: Discrediting the West, Diplomacy: Diplo...   \n",
       "3  ['URW: Blaming the war on others rather than t...   \n",
       "4  ['URW: Speculating war outcomes: Other', 'URW:...   \n",
       "\n",
       "                                                Text  \n",
       "0  Boris Johnson demands Putin ‘steps back from t...  \n",
       "1  Boris Johnson demands Putin ‘steps back from t...  \n",
       "2  Russia-Ukraine war map: Where are Russian troo...  \n",
       "3  NATO ‘Cautiously Optimistic’ Amid Reports of R...  \n",
       "4  Putin may ABANDON siege of Kyiv and try to bli...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df[\"High_Level_Narrative\"].str.startswith(\"URW\")].reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 264 URW High Level Narratives after spliting multiple URW High Level Narratives in multiple rows\n"
     ]
    }
   ],
   "source": [
    "print(f'We have {len(df)} URW High Level Narratives after spliting multiple URW High Level Narratives in multiple rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_ID</th>\n",
       "      <th>High_Level_Narrative</th>\n",
       "      <th>Sub_Narratives_List</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EN_UA_000923.txt</td>\n",
       "      <td>URW: Speculating war outcomes</td>\n",
       "      <td>['URW: Discrediting the West, Diplomacy: The E...</td>\n",
       "      <td>URW: Speculating war outcomes | Boris Johnson ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EN_UA_000923.txt</td>\n",
       "      <td>URW: Discrediting the West, Diplomacy</td>\n",
       "      <td>['URW: Discrediting the West, Diplomacy: The E...</td>\n",
       "      <td>URW: Discrediting the West, Diplomacy | Boris ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EN_UA_001032.txt</td>\n",
       "      <td>URW: Discrediting the West, Diplomacy</td>\n",
       "      <td>['URW: Discrediting the West, Diplomacy: Diplo...</td>\n",
       "      <td>URW: Discrediting the West, Diplomacy | Russia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EN_UA_001052.txt</td>\n",
       "      <td>URW: Blaming the war on others rather than the...</td>\n",
       "      <td>['URW: Blaming the war on others rather than t...</td>\n",
       "      <td>URW: Blaming the war on others rather than the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EN_UA_002668.txt</td>\n",
       "      <td>URW: Speculating war outcomes</td>\n",
       "      <td>['URW: Speculating war outcomes: Other', 'URW:...</td>\n",
       "      <td>URW: Speculating war outcomes | Putin may ABAN...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Document_ID                               High_Level_Narrative  \\\n",
       "0  EN_UA_000923.txt                      URW: Speculating war outcomes   \n",
       "1  EN_UA_000923.txt              URW: Discrediting the West, Diplomacy   \n",
       "2  EN_UA_001032.txt              URW: Discrediting the West, Diplomacy   \n",
       "3  EN_UA_001052.txt  URW: Blaming the war on others rather than the...   \n",
       "4  EN_UA_002668.txt                      URW: Speculating war outcomes   \n",
       "\n",
       "                                 Sub_Narratives_List  \\\n",
       "0  ['URW: Discrediting the West, Diplomacy: The E...   \n",
       "1  ['URW: Discrediting the West, Diplomacy: The E...   \n",
       "2  ['URW: Discrediting the West, Diplomacy: Diplo...   \n",
       "3  ['URW: Blaming the war on others rather than t...   \n",
       "4  ['URW: Speculating war outcomes: Other', 'URW:...   \n",
       "\n",
       "                                                Text  \n",
       "0  URW: Speculating war outcomes | Boris Johnson ...  \n",
       "1  URW: Discrediting the West, Diplomacy | Boris ...  \n",
       "2  URW: Discrediting the West, Diplomacy | Russia...  \n",
       "3  URW: Blaming the war on others rather than the...  \n",
       "4  URW: Speculating war outcomes | Putin may ABAN...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#in here we will add high level narrative to text with a | seperating high level narrative and text\n",
    "df[\"Text\"] = df[\"High_Level_Narrative\"] + \" | \" + df[\"Text\"]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                 [URW: Speculating war outcomes: Other]\n",
       "1      [URW: Discrediting the West, Diplomacy: The EU...\n",
       "2      [URW: Discrediting the West, Diplomacy: Diplom...\n",
       "3      [URW: Blaming the war on others rather than th...\n",
       "4      [URW: Speculating war outcomes: Other, URW: Sp...\n",
       "                             ...                        \n",
       "259    [URW: Praise of Russia: Praise of Russian mili...\n",
       "260    [URW: Amplifying war-related fears: By continu...\n",
       "261    [URW: Negative Consequences for the West: Sanc...\n",
       "262    [URW: Discrediting the West, Diplomacy: Diplom...\n",
       "263                   [URW: Russia is the Victim: Other]\n",
       "Name: Filtered_Sub_Narratives, Length: 264, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to filter sub-narratives that match the high-level narrative\n",
    "def filter_sub_narratives(row):\n",
    "    high_level = row[\"High_Level_Narrative\"].strip()\n",
    "    sub_narratives = eval(row[\"Sub_Narratives_List\"])  # Convert string representation of list to actual list\n",
    "    \n",
    "    # Keep only sub-narratives that start with the high-level narrative\n",
    "    filtered = [sub for sub in sub_narratives if sub.startswith(high_level)]\n",
    "    return filtered\n",
    "\n",
    "# Apply filtering function\n",
    "df[\"Filtered_Sub_Narratives\"] = df.apply(filter_sub_narratives, axis=1)\n",
    "df['Filtered_Sub_Narratives']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all unique sub-narratives\n",
    "sub_narratives = list(set(label for sublist in df[\"Filtered_Sub_Narratives\"] for label in sublist))\n",
    "len(sub_narratives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_ID</th>\n",
       "      <th>High_Level_Narrative</th>\n",
       "      <th>Sub_Narratives_List</th>\n",
       "      <th>Text</th>\n",
       "      <th>Filtered_Sub_Narratives</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EN_UA_000923.txt</td>\n",
       "      <td>URW: Speculating war outcomes</td>\n",
       "      <td>['URW: Discrediting the West, Diplomacy: The E...</td>\n",
       "      <td>URW: Speculating war outcomes | Boris Johnson ...</td>\n",
       "      <td>[URW: Speculating war outcomes: Other]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EN_UA_000923.txt</td>\n",
       "      <td>URW: Discrediting the West, Diplomacy</td>\n",
       "      <td>['URW: Discrediting the West, Diplomacy: The E...</td>\n",
       "      <td>URW: Discrediting the West, Diplomacy | Boris ...</td>\n",
       "      <td>[URW: Discrediting the West, Diplomacy: The EU...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EN_UA_001032.txt</td>\n",
       "      <td>URW: Discrediting the West, Diplomacy</td>\n",
       "      <td>['URW: Discrediting the West, Diplomacy: Diplo...</td>\n",
       "      <td>URW: Discrediting the West, Diplomacy | Russia...</td>\n",
       "      <td>[URW: Discrediting the West, Diplomacy: Diplom...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EN_UA_001052.txt</td>\n",
       "      <td>URW: Blaming the war on others rather than the...</td>\n",
       "      <td>['URW: Blaming the war on others rather than t...</td>\n",
       "      <td>URW: Blaming the war on others rather than the...</td>\n",
       "      <td>[URW: Blaming the war on others rather than th...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EN_UA_002668.txt</td>\n",
       "      <td>URW: Speculating war outcomes</td>\n",
       "      <td>['URW: Speculating war outcomes: Other', 'URW:...</td>\n",
       "      <td>URW: Speculating war outcomes | Putin may ABAN...</td>\n",
       "      <td>[URW: Speculating war outcomes: Other, URW: Sp...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Document_ID                               High_Level_Narrative  \\\n",
       "0  EN_UA_000923.txt                      URW: Speculating war outcomes   \n",
       "1  EN_UA_000923.txt              URW: Discrediting the West, Diplomacy   \n",
       "2  EN_UA_001032.txt              URW: Discrediting the West, Diplomacy   \n",
       "3  EN_UA_001052.txt  URW: Blaming the war on others rather than the...   \n",
       "4  EN_UA_002668.txt                      URW: Speculating war outcomes   \n",
       "\n",
       "                                 Sub_Narratives_List  \\\n",
       "0  ['URW: Discrediting the West, Diplomacy: The E...   \n",
       "1  ['URW: Discrediting the West, Diplomacy: The E...   \n",
       "2  ['URW: Discrediting the West, Diplomacy: Diplo...   \n",
       "3  ['URW: Blaming the war on others rather than t...   \n",
       "4  ['URW: Speculating war outcomes: Other', 'URW:...   \n",
       "\n",
       "                                                Text  \\\n",
       "0  URW: Speculating war outcomes | Boris Johnson ...   \n",
       "1  URW: Discrediting the West, Diplomacy | Boris ...   \n",
       "2  URW: Discrediting the West, Diplomacy | Russia...   \n",
       "3  URW: Blaming the war on others rather than the...   \n",
       "4  URW: Speculating war outcomes | Putin may ABAN...   \n",
       "\n",
       "                             Filtered_Sub_Narratives  \\\n",
       "0             [URW: Speculating war outcomes: Other]   \n",
       "1  [URW: Discrediting the West, Diplomacy: The EU...   \n",
       "2  [URW: Discrediting the West, Diplomacy: Diplom...   \n",
       "3  [URW: Blaming the war on others rather than th...   \n",
       "4  [URW: Speculating war outcomes: Other, URW: Sp...   \n",
       "\n",
       "                                              Labels  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_multi_label(data, narratives):\n",
    "    label_vectors = []\n",
    "    for narratives_list in data['Filtered_Sub_Narratives']:\n",
    "        vector = [1 if narrative in narratives_list else 0 for narrative in narratives]\n",
    "        label_vectors.append(vector)\n",
    "    data['Labels'] = label_vectors\n",
    "    return data\n",
    "\n",
    "# Encode sub-level narratives\n",
    "df = preprocess_multi_label(df, sub_narratives)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_rare_classes(data, labels, min_samples=15):\n",
    "    label_sums = np.sum(labels, axis=0)\n",
    "    rare_classes = np.where(label_sums < min_samples)[0]\n",
    "\n",
    "    for rare_class in rare_classes:\n",
    "        rare_indices = [i for i, lbl in enumerate(labels) if lbl[rare_class] == 1]\n",
    "        if len(rare_indices) > 0:\n",
    "            duplicate_data = data.iloc[rare_indices]\n",
    "            data = pd.concat([data] + [duplicate_data] * (min_samples - len(rare_indices)), ignore_index=True)\n",
    "            labels = np.vstack([labels] + [labels[rare_indices]] * (min_samples - len(rare_indices)))\n",
    "    return data, labels\n",
    "\n",
    "# Apply rare class handling\n",
    "labels = np.array(df['Labels'].tolist(), dtype=np.float32)\n",
    "df, labels = handle_rare_classes(df, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Use Combined_Text as input (High-Level + Text)\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df['Text'], labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Convert to Hugging Face Dataset format\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "train_dataset = Dataset.from_dict({\"text\": train_texts.tolist(), \"labels\": train_labels.tolist()})\n",
    "test_dataset = Dataset.from_dict({\"text\": test_texts.tolist(), \"labels\": test_labels.tolist()})\n",
    "datasets = DatasetDict({\"train\": train_dataset, \"test\": test_dataset})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"roberta-base\", num_labels=labels.shape[1], problem_type=\"multi_label_classification\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4a52d7c231e49c5997bb58ac7b6e30c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1003 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed30da3058bc41f7964fe87401b4bf60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/251 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_datasets = datasets.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from transformers import EvalPrediction\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    # Apply sigmoid activation to model predictions (logits)\n",
    "    sigmoid_preds = torch.sigmoid(torch.tensor(p.predictions)).numpy()\n",
    "\n",
    "    # Convert probabilities to binary predictions using a threshold of 0.5\n",
    "    preds = (sigmoid_preds > 0.5).astype(int)\n",
    "\n",
    "    # Ground-truth labels\n",
    "    labels = p.label_ids\n",
    "\n",
    "    # Calculate weighted precision, recall, and F1-score\n",
    "    precision = precision_score(labels, preds, average=\"weighted\", zero_division=0)\n",
    "    recall = recall_score(labels, preds, average=\"weighted\", zero_division=0)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\", zero_division=0)\n",
    "\n",
    "    return {\"precision\": precision, \"recall\": recall, \"f1\": f1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\makan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=10,  # Reduced from 20 for efficiency\n",
    "    learning_rate=2e-5, \n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1',\n",
    "    greater_is_better=True,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    save_total_limit=1,\n",
    "    seed=42,\n",
    "    weight_decay=0.01,\n",
    "    # fp16=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\makan\\AppData\\Local\\Temp\\ipykernel_20124\\69762720.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c583d21905f4aa2b4f697198c032fbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2510 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3443, 'grad_norm': 0.4358736276626587, 'learning_rate': 1.920318725099602e-05, 'epoch': 0.4}\n",
      "{'loss': 0.1729, 'grad_norm': 0.3660593628883362, 'learning_rate': 1.8406374501992033e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62334f204b664416ae69db7fb7a48a6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.15750738978385925, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_runtime': 4.5574, 'eval_samples_per_second': 55.075, 'eval_steps_per_second': 13.824, 'epoch': 1.0}\n",
      "{'loss': 0.1551, 'grad_norm': 0.3340104818344116, 'learning_rate': 1.760956175298805e-05, 'epoch': 1.2}\n",
      "{'loss': 0.1556, 'grad_norm': 0.3193029463291168, 'learning_rate': 1.6812749003984067e-05, 'epoch': 1.59}\n",
      "{'loss': 0.1451, 'grad_norm': 0.2908485233783722, 'learning_rate': 1.601593625498008e-05, 'epoch': 1.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efdd158be1b04a17b8a2b49e8155de6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.14535662531852722, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_runtime': 4.6239, 'eval_samples_per_second': 54.283, 'eval_steps_per_second': 13.625, 'epoch': 2.0}\n",
      "{'loss': 0.138, 'grad_norm': 1.088565707206726, 'learning_rate': 1.5219123505976096e-05, 'epoch': 2.39}\n",
      "{'loss': 0.1275, 'grad_norm': 0.5130642056465149, 'learning_rate': 1.4422310756972113e-05, 'epoch': 2.79}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e238699d4f6434d98e8a4d937843410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.10890921205282211, 'eval_precision': 0.25699745547073793, 'eval_recall': 0.0916030534351145, 'eval_f1': 0.1332012457800709, 'eval_runtime': 4.5472, 'eval_samples_per_second': 55.199, 'eval_steps_per_second': 13.855, 'epoch': 3.0}\n",
      "{'loss': 0.1106, 'grad_norm': 0.3714918792247772, 'learning_rate': 1.3625498007968127e-05, 'epoch': 3.19}\n",
      "{'loss': 0.1025, 'grad_norm': 0.4251931309700012, 'learning_rate': 1.2828685258964144e-05, 'epoch': 3.59}\n",
      "{'loss': 0.0959, 'grad_norm': 0.3303582966327667, 'learning_rate': 1.2031872509960161e-05, 'epoch': 3.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "517a5ef668da41aa81940dd49fcdb915",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.08611315488815308, 'eval_precision': 0.5765714824238998, 'eval_recall': 0.4020356234096692, 'eval_f1': 0.45963643860392844, 'eval_runtime': 4.488, 'eval_samples_per_second': 55.927, 'eval_steps_per_second': 14.037, 'epoch': 4.0}\n",
      "{'loss': 0.0837, 'grad_norm': 0.24056920409202576, 'learning_rate': 1.1235059760956175e-05, 'epoch': 4.38}\n",
      "{'loss': 0.0789, 'grad_norm': 0.2558727562427521, 'learning_rate': 1.0438247011952192e-05, 'epoch': 4.78}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ac1d52acf1444af900ec893f7ee3f43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.07193789631128311, 'eval_precision': 0.7080676679714184, 'eval_recall': 0.544529262086514, 'eval_f1': 0.5918723887671324, 'eval_runtime': 4.5404, 'eval_samples_per_second': 55.281, 'eval_steps_per_second': 13.875, 'epoch': 5.0}\n",
      "{'loss': 0.073, 'grad_norm': 0.24863848090171814, 'learning_rate': 9.641434262948209e-06, 'epoch': 5.18}\n",
      "{'loss': 0.0702, 'grad_norm': 0.2795252799987793, 'learning_rate': 8.844621513944224e-06, 'epoch': 5.58}\n",
      "{'loss': 0.0666, 'grad_norm': 0.2290727198123932, 'learning_rate': 8.04780876494024e-06, 'epoch': 5.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da5f4d62f99b4823a997c606d14a3f49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.06334521621465683, 'eval_precision': 0.7813953787236229, 'eval_recall': 0.6870229007633588, 'eval_f1': 0.7192369165727738, 'eval_runtime': 4.6157, 'eval_samples_per_second': 54.38, 'eval_steps_per_second': 13.649, 'epoch': 6.0}\n",
      "{'loss': 0.0634, 'grad_norm': 0.3828502297401428, 'learning_rate': 7.250996015936256e-06, 'epoch': 6.37}\n",
      "{'loss': 0.0598, 'grad_norm': 0.38201504945755005, 'learning_rate': 6.454183266932272e-06, 'epoch': 6.77}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f54a48eda6a4971a4b81f7582d880c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.05758536979556084, 'eval_precision': 0.8573513019575695, 'eval_recall': 0.7837150127226463, 'eval_f1': 0.8041199757286696, 'eval_runtime': 4.5186, 'eval_samples_per_second': 55.548, 'eval_steps_per_second': 13.942, 'epoch': 7.0}\n",
      "{'loss': 0.0563, 'grad_norm': 0.30093902349472046, 'learning_rate': 5.657370517928288e-06, 'epoch': 7.17}\n",
      "{'loss': 0.0562, 'grad_norm': 0.3569957911968231, 'learning_rate': 4.860557768924303e-06, 'epoch': 7.57}\n",
      "{'loss': 0.0544, 'grad_norm': 0.23548771440982819, 'learning_rate': 4.06374501992032e-06, 'epoch': 7.97}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f7ebe2cfaa04f7fa5b1f36bb5b86cff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.05394032225012779, 'eval_precision': 0.8907270441119376, 'eval_recall': 0.811704834605598, 'eval_f1': 0.8352217269853094, 'eval_runtime': 4.6006, 'eval_samples_per_second': 54.558, 'eval_steps_per_second': 13.694, 'epoch': 8.0}\n",
      "{'loss': 0.0519, 'grad_norm': 0.24795736372470856, 'learning_rate': 3.2669322709163346e-06, 'epoch': 8.37}\n",
      "{'loss': 0.0527, 'grad_norm': 0.17897090315818787, 'learning_rate': 2.470119521912351e-06, 'epoch': 8.76}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ec50484e1e14437a65119bac718cd8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.05159544199705124, 'eval_precision': 0.8904191670865034, 'eval_recall': 0.8320610687022901, 'eval_f1': 0.8461285391671342, 'eval_runtime': 4.4797, 'eval_samples_per_second': 56.03, 'eval_steps_per_second': 14.063, 'epoch': 9.0}\n",
      "{'loss': 0.049, 'grad_norm': 0.23260082304477692, 'learning_rate': 1.6733067729083665e-06, 'epoch': 9.16}\n",
      "{'loss': 0.0506, 'grad_norm': 0.27713316679000854, 'learning_rate': 8.764940239043826e-07, 'epoch': 9.56}\n",
      "{'loss': 0.0496, 'grad_norm': 0.23332971334457397, 'learning_rate': 7.968127490039842e-08, 'epoch': 9.96}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df8d6c8651564861b2067ae313affabb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.05091176927089691, 'eval_precision': 0.9036980849455739, 'eval_recall': 0.8371501272264631, 'eval_f1': 0.8532962209857189, 'eval_runtime': 4.7068, 'eval_samples_per_second': 53.327, 'eval_steps_per_second': 13.385, 'epoch': 10.0}\n",
      "{'train_runtime': 733.1059, 'train_samples_per_second': 13.682, 'train_steps_per_second': 3.424, 'train_loss': 0.09838276600457757, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2510, training_loss=0.09838276600457757, metrics={'train_runtime': 733.1059, 'train_samples_per_second': 13.682, 'train_steps_per_second': 3.424, 'total_flos': 2639951667302400.0, 'train_loss': 0.09838276600457757, 'epoch': 10.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azinja be bad ro kari nadashte bashid!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score, accuracy_score, recall_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_ID</th>\n",
       "      <th>High_Level_Narrative</th>\n",
       "      <th>Sub_Narratives_List</th>\n",
       "      <th>Text</th>\n",
       "      <th>Filtered_Sub_Narratives</th>\n",
       "      <th>Encoded_Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EN_UA_000923.txt</td>\n",
       "      <td>URW: Speculating war outcomes</td>\n",
       "      <td>['URW: Discrediting the West, Diplomacy: The E...</td>\n",
       "      <td>URW: Speculating war outcomes | Boris Johnson ...</td>\n",
       "      <td>[URW: Speculating war outcomes: Other]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EN_UA_000923.txt</td>\n",
       "      <td>URW: Discrediting the West, Diplomacy</td>\n",
       "      <td>['URW: Discrediting the West, Diplomacy: The E...</td>\n",
       "      <td>URW: Discrediting the West, Diplomacy | Boris ...</td>\n",
       "      <td>[URW: Discrediting the West, Diplomacy: The EU...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EN_UA_001032.txt</td>\n",
       "      <td>URW: Discrediting the West, Diplomacy</td>\n",
       "      <td>['URW: Discrediting the West, Diplomacy: Diplo...</td>\n",
       "      <td>URW: Discrediting the West, Diplomacy | Russia...</td>\n",
       "      <td>[URW: Discrediting the West, Diplomacy: Diplom...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EN_UA_001052.txt</td>\n",
       "      <td>URW: Blaming the war on others rather than the...</td>\n",
       "      <td>['URW: Blaming the war on others rather than t...</td>\n",
       "      <td>URW: Blaming the war on others rather than the...</td>\n",
       "      <td>[URW: Blaming the war on others rather than th...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EN_UA_002668.txt</td>\n",
       "      <td>URW: Speculating war outcomes</td>\n",
       "      <td>['URW: Speculating war outcomes: Other', 'URW:...</td>\n",
       "      <td>URW: Speculating war outcomes | Putin may ABAN...</td>\n",
       "      <td>[URW: Speculating war outcomes: Other, URW: Sp...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Document_ID                               High_Level_Narrative  \\\n",
       "0  EN_UA_000923.txt                      URW: Speculating war outcomes   \n",
       "1  EN_UA_000923.txt              URW: Discrediting the West, Diplomacy   \n",
       "2  EN_UA_001032.txt              URW: Discrediting the West, Diplomacy   \n",
       "3  EN_UA_001052.txt  URW: Blaming the war on others rather than the...   \n",
       "4  EN_UA_002668.txt                      URW: Speculating war outcomes   \n",
       "\n",
       "                                 Sub_Narratives_List  \\\n",
       "0  ['URW: Discrediting the West, Diplomacy: The E...   \n",
       "1  ['URW: Discrediting the West, Diplomacy: The E...   \n",
       "2  ['URW: Discrediting the West, Diplomacy: Diplo...   \n",
       "3  ['URW: Blaming the war on others rather than t...   \n",
       "4  ['URW: Speculating war outcomes: Other', 'URW:...   \n",
       "\n",
       "                                                Text  \\\n",
       "0  URW: Speculating war outcomes | Boris Johnson ...   \n",
       "1  URW: Discrediting the West, Diplomacy | Boris ...   \n",
       "2  URW: Discrediting the West, Diplomacy | Russia...   \n",
       "3  URW: Blaming the war on others rather than the...   \n",
       "4  URW: Speculating war outcomes | Putin may ABAN...   \n",
       "\n",
       "                             Filtered_Sub_Narratives  \\\n",
       "0             [URW: Speculating war outcomes: Other]   \n",
       "1  [URW: Discrediting the West, Diplomacy: The EU...   \n",
       "2  [URW: Discrediting the West, Diplomacy: Diplom...   \n",
       "3  [URW: Blaming the war on others rather than th...   \n",
       "4  [URW: Speculating war outcomes: Other, URW: Sp...   \n",
       "\n",
       "                                      Encoded_Labels  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlb = MultiLabelBinarizer(classes=unique_labels)\n",
    "df[\"Encoded_Labels\"] = mlb.fit_transform(df[\"Filtered_Sub_Narratives\"]).tolist()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "df[\"Encoded_Labels\"] = df[\"Encoded_Labels\"].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "df = shuffle(df, random_state=42).reset_index(drop=True)\n",
    "\n",
    "train_size = int(0.8 * len(df))\n",
    "# Convert to list explicitly\n",
    "train_texts, train_labels = list(df[\"Text\"][:train_size]), list(df[\"Encoded_Labels\"][:train_size])\n",
    "test_texts, test_labels = list(df[\"Text\"][train_size:]), list(df[\"Encoded_Labels\"][train_size:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the tokenizer and model\n",
    "model_name = \"roberta-base\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(list(train_texts), truncation=True, padding=True, max_length=512)\n",
    "test_encodings = tokenizer(list(test_texts), truncation=True, padding=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NarrativeDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=42, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = NarrativeDataset(train_encodings, train_labels)\n",
    "test_dataset = NarrativeDataset(test_encodings, test_labels)\n",
    "\n",
    "# Define model\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=len(unique_labels))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\makan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=20,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    save_total_limit=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, accuracy_score, recall_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    preds = (logits > 0.5).astype(int)  # Convert logits to binary predictions for multi-label classification\n",
    "    labels = np.array(labels)  # Ensure labels are in array format\n",
    "\n",
    "    # Compute macro-averaged metrics\n",
    "    f1 = f1_score(labels, preds, average=\"macro\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    recall = recall_score(labels, preds, average=\"macro\")\n",
    "\n",
    "    return {\"f1\": f1, \"accuracy\": acc, \"recall\": recall}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "022e52144eeb4b1f8308406f4bbc960e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/540 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5459, 'grad_norm': 0.861284613609314, 'learning_rate': 4.9074074074074075e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3339, 'grad_norm': 0.5963953137397766, 'learning_rate': 4.814814814814815e-05, 'epoch': 0.74}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c4bc09b3db94575957faf117ce1251a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\makan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\makan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.23084905743598938, 'eval_f1': 0.0, 'eval_accuracy': 0.0, 'eval_recall': 0.0, 'eval_runtime': 0.9801, 'eval_samples_per_second': 54.077, 'eval_steps_per_second': 7.142, 'epoch': 1.0}\n",
      "{'loss': 0.2499, 'grad_norm': 0.45457470417022705, 'learning_rate': 4.722222222222222e-05, 'epoch': 1.11}\n",
      "{'loss': 0.1993, 'grad_norm': 0.34782975912094116, 'learning_rate': 4.62962962962963e-05, 'epoch': 1.48}\n",
      "{'loss': 0.178, 'grad_norm': 0.32758286595344543, 'learning_rate': 4.5370370370370374e-05, 'epoch': 1.85}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ec339a417c84dec994551ab77d26446",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\makan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\makan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.16114045679569244, 'eval_f1': 0.0, 'eval_accuracy': 0.0, 'eval_recall': 0.0, 'eval_runtime': 0.9701, 'eval_samples_per_second': 54.632, 'eval_steps_per_second': 7.216, 'epoch': 2.0}\n",
      "{'loss': 0.1549, 'grad_norm': 0.2748515009880066, 'learning_rate': 4.4444444444444447e-05, 'epoch': 2.22}\n",
      "{'loss': 0.1444, 'grad_norm': 0.2389254868030548, 'learning_rate': 4.351851851851852e-05, 'epoch': 2.59}\n",
      "{'loss': 0.1464, 'grad_norm': 0.26790982484817505, 'learning_rate': 4.259259259259259e-05, 'epoch': 2.96}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "769c01f6785240e4b261e103c1d15c8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\makan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\makan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1468096822500229, 'eval_f1': 0.0, 'eval_accuracy': 0.0, 'eval_recall': 0.0, 'eval_runtime': 0.9744, 'eval_samples_per_second': 54.395, 'eval_steps_per_second': 7.184, 'epoch': 3.0}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\makan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:2164\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2162\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2165\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2169\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\makan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:2527\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2521\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m   2522\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[0;32m   2524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2525\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2526\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m-> 2527\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   2528\u001b[0m ):\n\u001b[0;32m   2529\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2530\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[0;32m   2531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
