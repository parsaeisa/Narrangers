{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\makan\\AppData\\Local\\Temp\\ipykernel_14352\\1983782537.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_data['Label'] = filtered_data.apply(assign_label, axis=1)\n",
      "C:\\Users\\makan\\AppData\\Local\\Temp\\ipykernel_14352\\1983782537.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_data['Label'] = filtered_data.apply(assign_label, axis=1)\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers import EvalPrediction\n",
    "from datasets import Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Load the CSV file\n",
    "data_path = 'data_with_text.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "# Split the data into EN_CC and EN_UA\n",
    "data['Type'] = data['Document_ID'].apply(lambda x: 'EN_CC' if x.startswith('EN_CC') else 'EN_UA')\n",
    "\n",
    "def preprocess_data(data, narrative_type, narratives_list):\n",
    "    \"\"\"Preprocess data for a specific narrative type.\"\"\"\n",
    "    filtered_data = data[data['Type'] == narrative_type]\n",
    "    \n",
    "    # Assign labels (narratives or \"Other\")\n",
    "    def assign_label(row):\n",
    "        for narrative in narratives_list:\n",
    "            if narrative in row['High_Level_Narratives_List']:\n",
    "                return narrative\n",
    "        return \"Other\"\n",
    "\n",
    "    filtered_data['Label'] = filtered_data.apply(assign_label, axis=1)\n",
    "    \n",
    "    return filtered_data\n",
    "\n",
    "# Define high-level narratives for CC and UA\n",
    "cc_narratives = [\n",
    "    \"CC: Amplifying Climate Fears\",\n",
    "    \"CC: Climate change is beneficial\",\n",
    "    \"CC: Controversy about green technologies\",\n",
    "    \"CC: Criticism of climate movement\",\n",
    "    \"CC: Criticism of climate policies\",\n",
    "    \"CC: Criticism of institutions and authorities\",\n",
    "    \"CC: Downplaying climate change\",\n",
    "    \"CC: Green policies are geopolitical instruments\",\n",
    "    \"CC: Hidden plots by secret schemes of powerful groups\",\n",
    "    \"CC: Questioning the measurements and science\",\n",
    "]\n",
    "\n",
    "ua_narratives = [\n",
    "    \"URW: Amplifying war-related fears\",\n",
    "    \"URW: Blaming the war on others rather than the invader\",\n",
    "    \"URW: Discrediting Ukraine\",\n",
    "    \"URW: Discrediting the West, Diplomacy\",\n",
    "    \"URW: Distrust towards Media\",\n",
    "    \"URW: Hidden plots by secret schemes of powerful groups\",\n",
    "    \"URW: Negative Consequences for the West\",\n",
    "    \"URW: Overpraising the West\",\n",
    "    \"URW: Praise of Russia\",\n",
    "    \"URW: Russia is the Victim\",\n",
    "    \"URW: Speculating war outcomes\",\n",
    "]\n",
    "\n",
    "# Preprocess EN_CC and EN_UA datasets\n",
    "en_cc_data = preprocess_data(data, \"EN_CC\", cc_narratives)\n",
    "en_ua_data = preprocess_data(data, \"EN_UA\", ua_narratives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\makan\\AppData\\Local\\Temp\\ipykernel_14352\\233597943.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  en_cc_data['Label'] = en_cc_data['Label'].map(cc_label_to_id)\n",
      "C:\\Users\\makan\\AppData\\Local\\Temp\\ipykernel_14352\\233597943.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  en_ua_data['Label'] = en_ua_data['Label'].map(ua_label_to_id)\n"
     ]
    }
   ],
   "source": [
    "# Map labels to integers for CC and UA\n",
    "cc_label_to_id = {label: idx for idx, label in enumerate(cc_narratives + [\"Other\"])}\n",
    "ua_label_to_id = {label: idx for idx, label in enumerate(ua_narratives + [\"Other\"])}\n",
    "\n",
    "cc_id_to_label = {v: k for k, v in cc_label_to_id.items()}\n",
    "ua_id_to_label = {v: k for k, v in ua_label_to_id.items()}\n",
    "\n",
    "# Convert string labels to numeric labels\n",
    "en_cc_data['Label'] = en_cc_data['Label'].map(cc_label_to_id)\n",
    "en_ua_data['Label'] = en_ua_data['Label'].map(ua_label_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def calculate_class_weights(labels):\n",
    "    label_counts = Counter(labels)\n",
    "    total_samples = sum(label_counts.values())\n",
    "    class_weights = {label: total_samples / (len(label_counts) * count) for label, count in label_counts.items()}\n",
    "    return class_weights\n",
    "\n",
    "# Calculate class weights for EN_CC\n",
    "en_cc_labels = en_cc_data['Label'].tolist()\n",
    "cc_class_weights = calculate_class_weights(en_cc_labels)\n",
    "\n",
    "# Calculate class weights for EN_UA\n",
    "en_ua_labels = en_ua_data['Label'].tolist()\n",
    "ua_class_weights = calculate_class_weights(en_ua_labels)\n",
    "\n",
    "# Convert class weights to tensors for PyTorch\n",
    "cc_class_weights_tensor = torch.tensor([cc_class_weights[label] for label in sorted(cc_class_weights)], dtype=torch.float)\n",
    "ua_class_weights_tensor = torch.tensor([ua_class_weights[label] for label in sorted(ua_class_weights)], dtype=torch.float)\n",
    "\n",
    "# Define a custom loss function with class weights\n",
    "def create_loss_function(class_weights_tensor):\n",
    "    return torch.nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "\n",
    "# Custom loss functions for CC and UA\n",
    "cc_loss_function = create_loss_function(cc_class_weights_tensor)\n",
    "ua_loss_function = create_loss_function(ua_class_weights_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10    75\n",
       "3     27\n",
       "4     21\n",
       "2     19\n",
       "5     15\n",
       "0      7\n",
       "1      4\n",
       "6      4\n",
       "8      3\n",
       "9      1\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_cc_data['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_rare_classes(data, min_samples=2):\n",
    "    label_counts = data['Label'].value_counts()\n",
    "    rare_labels = label_counts[label_counts < min_samples].index\n",
    "    \n",
    "    # Duplicate rare class samples to ensure minimum count\n",
    "    rare_data = data[data['Label'].isin(rare_labels)]\n",
    "    for _ in range(min_samples - 1):\n",
    "        data = pd.concat([data, rare_data])\n",
    "    return data\n",
    "\n",
    "en_cc_data = handle_rare_classes(en_cc_data)\n",
    "en_ua_data = handle_rare_classes(en_ua_data)\n",
    "\n",
    "# Split data into train/test sets\n",
    "def split_data(df):\n",
    "    train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "        df['Text'], df['Label'], test_size=0.2, random_state=42, stratify=df['Label']\n",
    "    )\n",
    "    \n",
    "    train_data = Dataset.from_pandas(pd.DataFrame({'text': train_texts, 'label': train_labels}))\n",
    "    test_data = Dataset.from_pandas(pd.DataFrame({'text': test_texts, 'label': test_labels}))\n",
    "    \n",
    "    return DatasetDict({\"train\": train_data, \"test\": test_data})\n",
    "\n",
    "en_cc_dataset = split_data(en_cc_data)\n",
    "en_ua_dataset = split_data(en_ua_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f135e416fc44666ae19cce0619a144c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/141 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9909842594eb48278d14c08fec610941",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbb5645733d6455f864d1d9694583357",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/178 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d91d8c1cdd3c416a99eaffc919f49525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/45 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load RoBERTa tokenizer and model\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# Tokenize datasets\n",
    "en_cc_dataset = en_cc_dataset.map(tokenize_function, batched=True)\n",
    "en_ua_dataset = en_ua_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "def load_model_with_custom_loss(num_labels, loss_function):\n",
    "    model = RobertaForSequenceClassification.from_pretrained(\n",
    "        'roberta-base',\n",
    "        num_labels=num_labels\n",
    "    )\n",
    "    model.classifier.loss_fn = loss_function\n",
    "    return model\n",
    "\n",
    "cc_model = load_model_with_custom_loss(len(cc_narratives) + 1, cc_loss_function)  # +1 for \"Other\"\n",
    "ua_model = load_model_with_custom_loss(len(ua_narratives) + 1, ua_loss_function)  # +1 for \"Other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\makan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\makan\\AppData\\Local\\Temp\\ipykernel_14352\\1048523157.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  cc_trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    f1 = f1_score(p.label_ids, preds, average=\"weighted\")\n",
    "    return {\"f1\": f1}\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    metric_for_best_model=\"f1\",\n",
    "    load_best_model_at_end=True,\n",
    "\n",
    "\n",
    ")\n",
    "\n",
    "# Train EN_CC model\n",
    "cc_trainer = Trainer(\n",
    "    model=cc_model,\n",
    "    args=training_args,\n",
    "    train_dataset=en_cc_dataset[\"train\"],\n",
    "    eval_dataset=en_cc_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9f131d118864f52a05cf9d5ad5ff885",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "601458f60e984094963104958207a63e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8192353248596191, 'eval_f1': 0.2450980392156863, 'eval_runtime': 0.9218, 'eval_samples_per_second': 39.055, 'eval_steps_per_second': 5.424, 'epoch': 1.0}\n",
      "{'loss': 2.1272, 'grad_norm': 4.997844219207764, 'learning_rate': 4.4444444444444447e-05, 'epoch': 1.11}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8eb2fe032d34eadb2fe8860c8fb37be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.770568609237671, 'eval_f1': 0.2450980392156863, 'eval_runtime': 0.9138, 'eval_samples_per_second': 39.395, 'eval_steps_per_second': 5.471, 'epoch': 2.0}\n",
      "{'loss': 1.7904, 'grad_norm': 4.2739949226379395, 'learning_rate': 3.888888888888889e-05, 'epoch': 2.22}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e0b3dfe13d74910b4dc615062f6472a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5790925025939941, 'eval_f1': 0.2450980392156863, 'eval_runtime': 0.9161, 'eval_samples_per_second': 39.296, 'eval_steps_per_second': 5.458, 'epoch': 3.0}\n",
      "{'loss': 1.6059, 'grad_norm': 6.627928733825684, 'learning_rate': 3.3333333333333335e-05, 'epoch': 3.33}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e39ccaebc7ed4ecc986cfa5acfc2c2f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4174120426177979, 'eval_f1': 0.4437669376693767, 'eval_runtime': 0.915, 'eval_samples_per_second': 39.345, 'eval_steps_per_second': 5.465, 'epoch': 4.0}\n",
      "{'loss': 1.4024, 'grad_norm': 9.865017890930176, 'learning_rate': 2.777777777777778e-05, 'epoch': 4.44}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c458201344e842ac8709ac5d3089a889",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3948510885238647, 'eval_f1': 0.4839244839244839, 'eval_runtime': 0.9223, 'eval_samples_per_second': 39.034, 'eval_steps_per_second': 5.421, 'epoch': 5.0}\n",
      "{'loss': 0.9872, 'grad_norm': 10.181580543518066, 'learning_rate': 2.2222222222222223e-05, 'epoch': 5.56}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d83c1ddf7984d42a6ebce439ce060f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3551788330078125, 'eval_f1': 0.49409722222222224, 'eval_runtime': 0.9221, 'eval_samples_per_second': 39.043, 'eval_steps_per_second': 5.423, 'epoch': 6.0}\n",
      "{'loss': 0.9009, 'grad_norm': 6.556413173675537, 'learning_rate': 1.6666666666666667e-05, 'epoch': 6.67}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d43cef28cdba4453b87c3aed15a382b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3347971439361572, 'eval_f1': 0.628747795414462, 'eval_runtime': 0.9059, 'eval_samples_per_second': 39.739, 'eval_steps_per_second': 5.519, 'epoch': 7.0}\n",
      "{'loss': 0.6669, 'grad_norm': 4.660386085510254, 'learning_rate': 1.1111111111111112e-05, 'epoch': 7.78}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bb0dfc61a4b4e18b900a953c54a276a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2475507259368896, 'eval_f1': 0.5161290322580645, 'eval_runtime': 0.9174, 'eval_samples_per_second': 39.239, 'eval_steps_per_second': 5.45, 'epoch': 8.0}\n",
      "{'loss': 0.5671, 'grad_norm': 6.02388858795166, 'learning_rate': 5.555555555555556e-06, 'epoch': 8.89}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b16cf779c6774832ac3b178b941a4e3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.295204758644104, 'eval_f1': 0.5309283309283309, 'eval_runtime': 0.9124, 'eval_samples_per_second': 39.454, 'eval_steps_per_second': 5.48, 'epoch': 9.0}\n",
      "{'loss': 0.4997, 'grad_norm': 4.068960189819336, 'learning_rate': 0.0, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c504874532c4c31af3668e610645e51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.232291579246521, 'eval_f1': 0.5859365849087544, 'eval_runtime': 0.9382, 'eval_samples_per_second': 38.372, 'eval_steps_per_second': 5.329, 'epoch': 10.0}\n",
      "{'train_runtime': 459.261, 'train_samples_per_second': 3.07, 'train_steps_per_second': 0.196, 'train_loss': 1.1719401677449544, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=90, training_loss=1.1719401677449544, metrics={'train_runtime': 459.261, 'train_samples_per_second': 3.07, 'train_steps_per_second': 0.196, 'total_flos': 371016566507520.0, 'train_loss': 1.1719401677449544, 'epoch': 10.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
