{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_ID</th>\n",
       "      <th>High_Level_Narratives_List</th>\n",
       "      <th>Sub_Narratives_List</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EN_CC_100000.txt</td>\n",
       "      <td>[CC: Controversy about green technologies, CC:...</td>\n",
       "      <td>[CC: Criticism of institutions and authorities...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EN_CC_100002.txt</td>\n",
       "      <td>[CC: Hidden plots by secret schemes of powerfu...</td>\n",
       "      <td>[CC: Criticism of institutions and authorities...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EN_CC_100003.txt</td>\n",
       "      <td>[CC: Criticism of climate movement, CC: Critic...</td>\n",
       "      <td>[CC: Criticism of climate movement: Ad hominem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EN_CC_100004.txt</td>\n",
       "      <td>[CC: Controversy about green technologies, CC:...</td>\n",
       "      <td>[CC: Controversy about green technologies: Ren...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EN_CC_100005.txt</td>\n",
       "      <td>[CC: Criticism of climate movement]</td>\n",
       "      <td>[CC: Criticism of climate movement: Other, CC:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>EN_UA_DEV_100028.txt</td>\n",
       "      <td>[URW: Negative Consequences for the West]</td>\n",
       "      <td>[URW: Negative Consequences for the West: Sanc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>EN_UA_DEV_216.txt</td>\n",
       "      <td>[URW: Negative Consequences for the West, URW:...</td>\n",
       "      <td>[URW: Discrediting the West, Diplomacy: The EU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>EN_UA_DEV_23.txt</td>\n",
       "      <td>[URW: Praise of Russia, URW: Distrust towards ...</td>\n",
       "      <td>[URW: Amplifying war-related fears: By continu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>EN_UA_DEV_24.txt</td>\n",
       "      <td>[URW: Negative Consequences for the West, URW:...</td>\n",
       "      <td>[URW: Negative Consequences for the West: Sanc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>EN_UA_DEV_26.txt</td>\n",
       "      <td>[URW: Russia is the Victim]</td>\n",
       "      <td>[URW: Russia is the Victim: Other]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>399 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Document_ID                         High_Level_Narratives_List  \\\n",
       "0        EN_CC_100000.txt  [CC: Controversy about green technologies, CC:...   \n",
       "1        EN_CC_100002.txt  [CC: Hidden plots by secret schemes of powerfu...   \n",
       "2        EN_CC_100003.txt  [CC: Criticism of climate movement, CC: Critic...   \n",
       "3        EN_CC_100004.txt  [CC: Controversy about green technologies, CC:...   \n",
       "4        EN_CC_100005.txt                [CC: Criticism of climate movement]   \n",
       "..                    ...                                                ...   \n",
       "394  EN_UA_DEV_100028.txt          [URW: Negative Consequences for the West]   \n",
       "395     EN_UA_DEV_216.txt  [URW: Negative Consequences for the West, URW:...   \n",
       "396      EN_UA_DEV_23.txt  [URW: Praise of Russia, URW: Distrust towards ...   \n",
       "397      EN_UA_DEV_24.txt  [URW: Negative Consequences for the West, URW:...   \n",
       "398      EN_UA_DEV_26.txt                        [URW: Russia is the Victim]   \n",
       "\n",
       "                                   Sub_Narratives_List  \n",
       "0    [CC: Criticism of institutions and authorities...  \n",
       "1    [CC: Criticism of institutions and authorities...  \n",
       "2    [CC: Criticism of climate movement: Ad hominem...  \n",
       "3    [CC: Controversy about green technologies: Ren...  \n",
       "4    [CC: Criticism of climate movement: Other, CC:...  \n",
       "..                                                 ...  \n",
       "394  [URW: Negative Consequences for the West: Sanc...  \n",
       "395  [URW: Discrediting the West, Diplomacy: The EU...  \n",
       "396  [URW: Amplifying war-related fears: By continu...  \n",
       "397  [URW: Negative Consequences for the West: Sanc...  \n",
       "398                 [URW: Russia is the Victim: Other]  \n",
       "\n",
       "[399 rows x 3 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "file_path = 'data\\EN\\subtask-2-annotations.txt'\n",
    "df = pd.read_csv(file_path, sep=\"\\t\", header=None, names=[\"Document_ID\", \"High_Level_Narratives\", \"Sub_Narratives\"])\n",
    "\n",
    "df = df.groupby(\"Document_ID\").agg({\n",
    "    \"High_Level_Narratives\": lambda x: list(set(\";\".join(x).split(\";\"))),\n",
    "    \"Sub_Narratives\": lambda x: list(set(\";\".join(x).split(\";\")))\n",
    "}).reset_index()\n",
    "\n",
    "df.rename(columns={\n",
    "    \"High_Level_Narratives\": \"High_Level_Narratives_List\",\n",
    "    \"Sub_Narratives\": \"Sub_Narratives_List\"\n",
    "}, inplace=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Binary_Label\n",
       "1    230\n",
       "0    169\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add a Binary Classification Column\n",
    "def classify_binary(narratives):\n",
    "    if any(narrative.startswith(('URW', 'CC')) for narrative in narratives):\n",
    "        return 1  # Non-Other\n",
    "    return 0  # Other\n",
    "\n",
    "# Apply the classification to High_Level_Narratives_List\n",
    "df['Binary_Label'] = df['High_Level_Narratives_List'].apply(classify_binary)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "df.value_counts('Binary_Label')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_ID</th>\n",
       "      <th>High_Level_Narratives_List</th>\n",
       "      <th>Sub_Narratives_List</th>\n",
       "      <th>Binary_Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EN_CC_100000.txt</td>\n",
       "      <td>[CC: Controversy about green technologies, CC:...</td>\n",
       "      <td>[CC: Criticism of institutions and authorities...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EN_CC_100002.txt</td>\n",
       "      <td>[CC: Hidden plots by secret schemes of powerfu...</td>\n",
       "      <td>[CC: Criticism of institutions and authorities...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EN_CC_100003.txt</td>\n",
       "      <td>[CC: Criticism of climate movement, CC: Critic...</td>\n",
       "      <td>[CC: Criticism of climate movement: Ad hominem...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EN_CC_100004.txt</td>\n",
       "      <td>[CC: Controversy about green technologies, CC:...</td>\n",
       "      <td>[CC: Controversy about green technologies: Ren...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EN_CC_100005.txt</td>\n",
       "      <td>[CC: Criticism of climate movement]</td>\n",
       "      <td>[CC: Criticism of climate movement: Other, CC:...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Document_ID                         High_Level_Narratives_List  \\\n",
       "0  EN_CC_100000.txt  [CC: Controversy about green technologies, CC:...   \n",
       "1  EN_CC_100002.txt  [CC: Hidden plots by secret schemes of powerfu...   \n",
       "2  EN_CC_100003.txt  [CC: Criticism of climate movement, CC: Critic...   \n",
       "3  EN_CC_100004.txt  [CC: Controversy about green technologies, CC:...   \n",
       "4  EN_CC_100005.txt                [CC: Criticism of climate movement]   \n",
       "\n",
       "                                 Sub_Narratives_List  Binary_Label  \n",
       "0  [CC: Criticism of institutions and authorities...             1  \n",
       "1  [CC: Criticism of institutions and authorities...             1  \n",
       "2  [CC: Criticism of climate movement: Ad hominem...             1  \n",
       "3  [CC: Controversy about green technologies: Ren...             1  \n",
       "4  [CC: Criticism of climate movement: Other, CC:...             1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing documents: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_ID</th>\n",
       "      <th>High_Level_Narratives_List</th>\n",
       "      <th>Sub_Narratives_List</th>\n",
       "      <th>Binary_Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EN_CC_100000.txt</td>\n",
       "      <td>[CC: Controversy about green technologies, CC:...</td>\n",
       "      <td>[CC: Criticism of institutions and authorities...</td>\n",
       "      <td>1</td>\n",
       "      <td>Pentagon plans to serve LAB-GROWN MEAT to troo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EN_CC_100002.txt</td>\n",
       "      <td>[CC: Hidden plots by secret schemes of powerfu...</td>\n",
       "      <td>[CC: Criticism of institutions and authorities...</td>\n",
       "      <td>1</td>\n",
       "      <td>Oxford Residents Mount Resistance Against the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EN_CC_100003.txt</td>\n",
       "      <td>[CC: Criticism of climate movement, CC: Critic...</td>\n",
       "      <td>[CC: Criticism of climate movement: Ad hominem...</td>\n",
       "      <td>1</td>\n",
       "      <td>Fonda Heads To Canada For Oil Sands Protest, M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EN_CC_100004.txt</td>\n",
       "      <td>[CC: Controversy about green technologies, CC:...</td>\n",
       "      <td>[CC: Controversy about green technologies: Ren...</td>\n",
       "      <td>1</td>\n",
       "      <td>A Tesla Owner Just Exposed A Sick Secret About...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EN_CC_100005.txt</td>\n",
       "      <td>[CC: Criticism of climate movement]</td>\n",
       "      <td>[CC: Criticism of climate movement: Other, CC:...</td>\n",
       "      <td>1</td>\n",
       "      <td>Climate Crazies Fail in Attempt to Vandalize A...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Document_ID                         High_Level_Narratives_List  \\\n",
       "0  EN_CC_100000.txt  [CC: Controversy about green technologies, CC:...   \n",
       "1  EN_CC_100002.txt  [CC: Hidden plots by secret schemes of powerfu...   \n",
       "2  EN_CC_100003.txt  [CC: Criticism of climate movement, CC: Critic...   \n",
       "3  EN_CC_100004.txt  [CC: Controversy about green technologies, CC:...   \n",
       "4  EN_CC_100005.txt                [CC: Criticism of climate movement]   \n",
       "\n",
       "                                 Sub_Narratives_List  Binary_Label  \\\n",
       "0  [CC: Criticism of institutions and authorities...             1   \n",
       "1  [CC: Criticism of institutions and authorities...             1   \n",
       "2  [CC: Criticism of climate movement: Ad hominem...             1   \n",
       "3  [CC: Controversy about green technologies: Ren...             1   \n",
       "4  [CC: Criticism of climate movement: Other, CC:...             1   \n",
       "\n",
       "                                                Text  \n",
       "0  Pentagon plans to serve LAB-GROWN MEAT to troo...  \n",
       "1  Oxford Residents Mount Resistance Against the ...  \n",
       "2  Fonda Heads To Canada For Oil Sands Protest, M...  \n",
       "3  A Tesla Owner Just Exposed A Sick Secret About...  \n",
       "4  Climate Crazies Fail in Attempt to Vandalize A...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Path to the raw-documents folder\n",
    "documents_folder = r'data\\EN\\raw-documents'\n",
    "\n",
    "# Function to read all document texts and map to their IDs\n",
    "def load_documents(folder_path):\n",
    "    documents = {}\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\"):  # Ensure only text files are read\n",
    "            doc_id = filename  # Extract Document_ID from filename\n",
    "            with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
    "                documents[doc_id] = file.read().strip()\n",
    "    return documents\n",
    "\n",
    "# Load document texts into a dictionary\n",
    "document_texts = load_documents(documents_folder)\n",
    "\n",
    "# Map document texts to the DataFrame\n",
    "df['Text'] = df['Document_ID'].map(document_texts)\n",
    "\n",
    "# Check if all documents are successfully mapped\n",
    "print(f\"Number of missing documents: {df['Text'].isnull().sum()}\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=-1)  # Get predicted class\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average=\"binary\")\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "    }\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Load RoBERTa tokenizer and model\n",
    "MODEL_NAME = \"roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)  # Binary classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43e57b7afb9a4730b1977454f204da19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/399 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Convert DataFrame to Hugging Face Dataset\n",
    "hf_dataset = Dataset.from_pandas(df[['Text', 'Binary_Label']])\n",
    "\n",
    "# Tokenize the text data\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['Text'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = hf_dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into train and test sets\n",
    "train_test_split = tokenized_datasets.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = train_test_split['train']\n",
    "test_dataset = train_test_split['test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 319\n",
      "Number of testing examples: 80\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training examples: {len(train_dataset)}\")\n",
    "print(f\"Number of testing examples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: tensor([1.1805, 0.8674])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Assuming your labels are in a column called 'Binary_Label' in a DataFrame\n",
    "class_labels = df['Binary_Label'].values\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced', \n",
    "    classes=np.unique(class_labels), \n",
    "    y=class_labels\n",
    ")\n",
    "\n",
    "# Convert to a PyTorch tensor\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "print(\"Class weights:\", class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# Update the forward pass to include weighted loss\n",
    "class WeightedRobertaModel(torch.nn.Module):\n",
    "    def __init__(self, model, class_weights):\n",
    "        super(WeightedRobertaModel, self).__init__()\n",
    "        self.model = model\n",
    "        self.class_weights = class_weights\n",
    "        self.loss_fn = CrossEntropyLoss(weight=self.class_weights)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.loss_fn(logits, labels)\n",
    "        return {\"loss\": loss, \"logits\": logits} if labels is not None else {\"logits\": logits}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_model = WeightedRobertaModel(model, class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\makan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# from transformers import TrainingArguments\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./results\",  # Directory to save results\n",
    "#     evaluation_strategy=\"epoch\",  # Evaluate at the end of each epoch\n",
    "#     save_strategy=\"epoch\",  # Save at the end of each epoch\n",
    "#     learning_rate=1e-5,  # Fine-tuning learning rate\n",
    "#     per_device_train_batch_size=12,  # Batch size per GPU\n",
    "#     per_device_eval_batch_size=16,\n",
    "#     num_train_epochs=10,  # Number of epochs\n",
    "#     # weight_decay=0.01,  # L2 regularization\n",
    "#     logging_dir=\"./logs\",  # Directory for logs\n",
    "#     logging_steps=10,\n",
    "#     load_best_model_at_end=True  # Load the best model after training\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',                    # Directory to store checkpoints and final model\n",
    "    num_train_epochs=10,                       # Total number of training epochs\n",
    "    learning_rate=2e-5,  # Fine-tuning learning rate\n",
    "    per_device_train_batch_size=10,            # Batch size per device during training\n",
    "    per_device_eval_batch_size=10,             # Batch size for evaluation\n",
    "    evaluation_strategy='epoch',               # Evaluate at the end of each epoch\n",
    "    save_strategy='epoch',                     # Save model at the end of each epoch\n",
    "    load_best_model_at_end=True,               # Load the best model at the end of training\n",
    "    metric_for_best_model='f1',                # Use F1 score to evaluate the best model\n",
    "    greater_is_better=True,                    # Higher F1 is better\n",
    "    logging_dir='./logs',                      # Directory for storing logs\n",
    "    logging_steps=100,                         # Log every 100 steps\n",
    "    save_total_limit=3,                        # Limit the total amount of checkpoints\n",
    "    seed=42,                                   # Seed for reproducibility\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Text', 'labels', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 80\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename the Binary_Label column to labels\n",
    "train_dataset = train_dataset.rename_column(\"Binary_Label\", \"labels\")\n",
    "test_dataset = test_dataset.rename_column(\"Binary_Label\", \"labels\")\n",
    "test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff2380b810ba4979b1a8f9bdc7b90b4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/320 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be215ea557d04f83856e34a4549ebf8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6743009090423584, 'eval_accuracy': 0.6625, 'eval_precision': 0.6140350877192983, 'eval_recall': 0.875, 'eval_f1': 0.7216494845360825, 'eval_runtime': 1.4223, 'eval_samples_per_second': 56.246, 'eval_steps_per_second': 5.625, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e263b411e2234d108119fe6a64f22c5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5474344491958618, 'eval_accuracy': 0.75, 'eval_precision': 0.6851851851851852, 'eval_recall': 0.925, 'eval_f1': 0.7872340425531915, 'eval_runtime': 1.4277, 'eval_samples_per_second': 56.033, 'eval_steps_per_second': 5.603, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38ac7d9a7ec34ef1bbe4e2e3fd44a3b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5304887294769287, 'eval_accuracy': 0.7, 'eval_precision': 0.8333333333333334, 'eval_recall': 0.5, 'eval_f1': 0.625, 'eval_runtime': 1.4425, 'eval_samples_per_second': 55.459, 'eval_steps_per_second': 5.546, 'epoch': 3.0}\n",
      "{'loss': 0.5772, 'grad_norm': 12.742156028747559, 'learning_rate': 6.875e-06, 'epoch': 3.12}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e579e434ed594aacaade459cc90f2fd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.547718346118927, 'eval_accuracy': 0.75, 'eval_precision': 0.717391304347826, 'eval_recall': 0.825, 'eval_f1': 0.7674418604651163, 'eval_runtime': 1.4445, 'eval_samples_per_second': 55.384, 'eval_steps_per_second': 5.538, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d33dc23244c4e1995ee6ea963a4b5a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6109122037887573, 'eval_accuracy': 0.7625, 'eval_precision': 0.723404255319149, 'eval_recall': 0.85, 'eval_f1': 0.7816091954022989, 'eval_runtime': 1.4534, 'eval_samples_per_second': 55.043, 'eval_steps_per_second': 5.504, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2994b6f38c0f4ce094b17861acbaab32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.795811653137207, 'eval_accuracy': 0.7375, 'eval_precision': 0.7435897435897436, 'eval_recall': 0.725, 'eval_f1': 0.7341772151898734, 'eval_runtime': 1.442, 'eval_samples_per_second': 55.478, 'eval_steps_per_second': 5.548, 'epoch': 6.0}\n",
      "{'loss': 0.2788, 'grad_norm': 8.475090026855469, 'learning_rate': 3.7500000000000005e-06, 'epoch': 6.25}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a396531ef644b47a0d5bfba800a5836",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9868605732917786, 'eval_accuracy': 0.7125, 'eval_precision': 0.6976744186046512, 'eval_recall': 0.75, 'eval_f1': 0.7228915662650602, 'eval_runtime': 1.4493, 'eval_samples_per_second': 55.2, 'eval_steps_per_second': 5.52, 'epoch': 7.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feee09a6f4aa439a9832fcf30f0131e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0247923135757446, 'eval_accuracy': 0.7375, 'eval_precision': 0.6938775510204082, 'eval_recall': 0.85, 'eval_f1': 0.7640449438202247, 'eval_runtime': 1.4421, 'eval_samples_per_second': 55.475, 'eval_steps_per_second': 5.547, 'epoch': 8.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20aca692dcf14d688640781bc7527405",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0734596252441406, 'eval_accuracy': 0.75, 'eval_precision': 0.7083333333333334, 'eval_recall': 0.85, 'eval_f1': 0.7727272727272727, 'eval_runtime': 1.4439, 'eval_samples_per_second': 55.406, 'eval_steps_per_second': 5.541, 'epoch': 9.0}\n",
      "{'loss': 0.1345, 'grad_norm': 4.904441833496094, 'learning_rate': 6.25e-07, 'epoch': 9.38}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72a6c758cead43899259759a2656aafd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1277389526367188, 'eval_accuracy': 0.75, 'eval_precision': 0.717391304347826, 'eval_recall': 0.825, 'eval_f1': 0.7674418604651163, 'eval_runtime': 1.4231, 'eval_samples_per_second': 56.216, 'eval_steps_per_second': 5.622, 'epoch': 10.0}\n",
      "{'train_runtime': 212.3352, 'train_samples_per_second': 15.023, 'train_steps_per_second': 1.507, 'train_loss': 0.3229636371135712, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=320, training_loss=0.3229636371135712, metrics={'train_runtime': 212.3352, 'train_samples_per_second': 15.023, 'train_steps_per_second': 1.507, 'total_flos': 0.0, 'train_loss': 0.3229636371135712, 'epoch': 10.0})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "# Define Trainer\n",
    "trainer = Trainer(\n",
    "    model=  weighted_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb1acccf3ba74d0698abe278d9526959",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5474344491958618, 'eval_accuracy': 0.75, 'eval_precision': 0.6851851851851852, 'eval_recall': 0.925, 'eval_f1': 0.7872340425531915, 'eval_runtime': 1.7091, 'eval_samples_per_second': 46.809, 'eval_steps_per_second': 4.681, 'epoch': 10.0}\n"
     ]
    }
   ],
   "source": [
    "results = trainer.evaluate(eval_dataset=test_dataset)\n",
    "print(results)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
